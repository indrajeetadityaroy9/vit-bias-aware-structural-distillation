from __future__ import annotations

import torch
import torch.nn as nn
import torch.nn.functional as F


class BuresWassersteinLoss(nn.Module):
    """Bures-Wasserstein loss on token Gaussian statistics.

    Uses diagonal covariance when N < D, full covariance otherwise.
    """

    def __init__(self, *, eps: float = 1e-5):
        super().__init__()
        self.eps = eps

    def forward(
        self, student_tokens: torch.Tensor, teacher_tokens: torch.Tensor
    ) -> torch.Tensor:
        s = student_tokens.float()
        t = teacher_tokens.detach().float()

        # Align channel dimensions before moment matching.
        D_s, D_t = s.shape[2], t.shape[2]
        if D_s != D_t:
            target_d = min(D_s, D_t)
            if D_s > D_t:
                s = F.adaptive_avg_pool1d(s, target_d)
            else:
                t = F.adaptive_avg_pool1d(t, target_d)

        B, N, D = s.shape

        mu_s, mu_t = s.mean(1), t.mean(1)
        mean_loss = (mu_s - mu_t).pow(2).sum(-1).mean()

        s_c = s - mu_s.unsqueeze(1)
        t_c = t - mu_t.unsqueeze(1)

        if N < D:
            var_s = s_c.pow(2).mean(dim=1) + self.eps
            var_t = t_c.pow(2).mean(dim=1) + self.eps
            cov_loss = (var_s.sqrt() - var_t.sqrt()).pow(2).sum(-1).mean()
        else:
            eps_eye = self.eps * torch.eye(D, device=s.device, dtype=s.dtype).unsqueeze(0)
            cov_s = torch.bmm(s_c.transpose(1, 2), s_c) / N + eps_eye
            cov_t = torch.bmm(t_c.transpose(1, 2), t_c) / N + eps_eye

            L_s = torch.linalg.cholesky(cov_s)
            L_t = torch.linalg.cholesky(cov_t)

            nuclear = torch.linalg.svdvals(
                torch.bmm(L_t.transpose(1, 2), L_s)
            ).sum(dim=-1)
            trace_s = torch.diagonal(cov_s, dim1=-2, dim2=-1).sum(-1)
            trace_t = torch.diagonal(cov_t, dim1=-2, dim2=-1).sum(-1)
            cov_loss = (trace_s + trace_t - 2 * nuclear).clamp(min=0.0).mean()

        # Keep mean and covariance terms on comparable scales.
        with torch.no_grad():
            scale = (cov_loss / (mean_loss + 1e-8)).clamp(0.1, 10.0)

        return scale * mean_loss + cov_loss
