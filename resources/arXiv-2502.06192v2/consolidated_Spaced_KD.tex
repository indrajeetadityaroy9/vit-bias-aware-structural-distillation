%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{multirow}
\definecolor{darkgreen}{RGB}{5,102,8}

\usepackage{algorithm}
\usepackage{algorithmic}

%\usepackage[linenumbered, ruled, vlined]{algorithmic}


% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage{icml2025}

\newcommand{\sgl}[1]{{\color{red}{ [sgl: #1]}}}
\newcommand{\hw}[1]{{\color{brown}{ [hw: #1]}}}
\newcommand{\ly}[1]{{\color{blue}{ [ly: #1]}}}
\newcommand{\lb}[1]{{\color{pink}{ [lb: #1]}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Brain-inspired spacing effect that improves knowledge distillation.}

\begin{document}

\twocolumn[
    \icmltitle{\emph{Right Time to Learn}: Promoting Generalization via \\Bio-inspired Spacing Effect in Knowledge Distillation }
    
    % It is OKAY to include author information, even for blind
    % submissions: the style file will automatically remove it for you
    % unless you've provided the [accepted] option to the icml2025
    % package.
    
    % List of affiliations: The first argument should be a (short)
    % identifier you will use later to specify author affiliations
    % Academic affiliations should list Department, University, City, Region, Country
    % Industry affiliations should list Company, City, Region, Country
    
    % You can specify symbols, otherwise they are numbered in order.
    % Ideally, you should not use this facility. Affiliations will be numbered
    % in order of appearance and this is the preferred way.
    
    \icmlsetsymbol{equal}{*}
    
    \begin{icmlauthorlist}
    \icmlauthor{Guanglong Sun}{equal,IDG,tp,baai}
    \icmlauthor{Hongwei Yan}{equal,IDG,tp,baai}
    \icmlauthor{Liyuan Wang}{cs}
    \icmlauthor{Qian Li}{IDG}
    \icmlauthor{Bo Lei}{baai}
    \icmlauthor{Yi Zhong}{IDG,tp}
    % \icmlauthor{Firstname7 Lastname7}{comp}
    %\icmlauthor{}{sch}
    % \icmlauthor{Firstname8 Lastname8}{sch}
    % \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
    \end{icmlauthorlist}
    
    \icmlaffiliation{IDG}{School of Life Sciences, IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China}
    \icmlaffiliation{cs}{Dept. of Comp. Sci. \& Tech., Institute for AI, BNRist Center, Beijing, China}
    %\icmlaffiliation{th}{Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Beijing, China}
    \icmlaffiliation{baai}{Beijing Academy of Artificial Intelligence, Beijing, China}
    \icmlaffiliation{tp}{Tsinghua–Peking Joint Center for Life Sciences}
    
    % \icmlcorrespondingauthor{Yi Zhong}{zhongyithu@tsinghua.edu.cn}
    \icmlcorrespondingauthor{Liyuan Wang}{wly2023@tsinghua.edu.cn}
    
    % You may provide any keywords that you
    % find helpful for describing your paper; these are used to populate
    % the "keywords" metadata in the PDF but will not be shown in the document
    \icmlkeywords{Knowledge Distillation, Brain-inspired Al, Machine Learning, Spacing effect}
    
    \vskip 0.15in
    ]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} 

% otherwise use the standard text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract
\vspace{0.5cm}
\begin{abstract}
Knowledge distillation (KD) is a powerful strategy for training deep neural networks (DNNs). Although it was originally proposed to train a more compact ``student'' model from a large ``teacher'' model, many recent efforts have focused on adapting it to promote generalization of the model itself, such as online KD and self KD. % as an effective way
Here, we propose an accessible and compatible strategy named Spaced KD to improve the effectiveness of both online KD and self KD, in which the student model distills knowledge from a teacher model trained with a space interval ahead. This strategy is inspired by a prominent theory named \emph{spacing effect} in biological learning and memory, positing that appropriate intervals between learning trials can significantly enhance learning performance. 
With both theoretical and empirical analyses, we demonstrate that the benefits of the proposed Spaced KD stem from convergence to a flatter loss landscape during stochastic gradient descent (SGD).
We perform extensive experiments to validate the effectiveness of Spaced KD in improving the learning performance of DNNs (e.g., the performance gain is up to 2.31\% and 3.34\% on Tiny-ImageNet over online KD and self KD, respectively). Our codes have been released on github~\url{https://github.com/SunGL001/Spaced-KD}.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
\section{Introduction}\label{sec:intro}

Knowledge distillation (KD) is a powerful technique to transfer knowledge between deep neural networks (DNNs)~\citep{review_KDSurvey,review_ST}. %, including visual recognition, speech recognition, natural language processing, and recommendation systems
Despite its extensive applications to construct a more compact ``student'' model from a converged large ``teacher'' model (aka offline KD), there have been many recent efforts using KD to promote generalization of the model itself, such as online KD~\citep{deepMutual, ONE,multiTeacher} 
%li2020explicit,liu2021semi,wu2021peer
and self KD~\citep{self-kd,mobahi2020self}.
%yang2019snapshot,zhang2020self,lee2019rethinking,phuong2019distillation,lan2019self
Specifically, online KD simplifies the KD process by training the teacher and the student simultaneously, while self KD involves using the same network as both teacher and student. However, as these paradigms can only moderately improve learning performance, how to design a more desirable KD paradigm in terms of generalization remains an open question.
%whether the structure of such learning procedure can be further optimized to achieve improvement of learning efficiency.

\begin{figure}
    \centering
     \vspace{+.2cm}
    \includegraphics[width=0.8\linewidth]{./imgs/figs/fig1_diagram.pdf}
    \vspace{-.1cm}
    \caption{\textbf{Diagram of Spaced KD.} In online KD, the teacher and student are two individual networks. In self KD, we follow the prior work~\citep{self-kd} that distills knowledge from the deepest layer to the shallower layers of the same network. In Spaced KD, we train the teacher with a controllable space interval steps ahead and then distill its knowledge to the student network.
    %Thus, the teacher is the deepest layer of the network and the students are the shallow layers along with their corresponding classifiers.
    }
    \label{fig:diagram}
    \vspace{-.4cm}
\end{figure}

Compared to DNNs, biological neural networks (BNNs) are advantageous in learning and generalization with specialized adaptation mechanisms and effective learning procedures. In particular, it is commonly recognized that extending the interval between individual learning events can considerably enhance the learning performance, known as the \emph{spacing effect}~\citep{ebbingham1913memory,smolen2016right}. This highlights the benefits of spaced study sessions for improving the efficiency of learning compared to continuous sessions, and has been described across a wide range of species from invertebrates to humans~\citep{beck2000learning,pagani2009phosphatase,menzel2001massed,anderson2008spaced,bello2013differential,medin1974comparative,robbins1973memory}.%\ly{modify the presentation here, especially the ``memory''} 
Taking human learning as an example, the spacing effect could enhance skill and motor learning~\citep{donovan1999meta,shea2000spacing}, classroom education~\citep{gluckman2014spacing,roediger2008learning,sobel2011spacing}, and the generalization of conceptual knowledge in children~\citep{vlach2014spacing}. %Furthermore, various computational cognitive models suggest that memory traces become stronger and more resistant to decay following spaced trials~\citep{landauer1969reinforcement,peterson1966short,wickelgren1972trace}.


%Online KD proposes to train both teacher and student from scratch simultaneously with the same source of training data, showing an effective solution to develop real-world KD applications. 
%the convergent principle of g
Inspired by biological learning, we propose to incorporate such spacing effect into KD (referred to as Spaced KD, see Fig.~\ref{fig:diagram}) as a general strategy to promote the generalization of DNNs (see Fig.~\ref{fig:main_result}). We first provide an in-depth theoretical analysis of the potential benefits of Spaced KD. Compared to regular KD strategies, the proposed Spaced KD helps DNNs find a flat minima during stochastic gradient descent (SGD)~\citep{sutskever2013importance}, which has proven to be closely related to generalization. %We then empirically investigate the impact of learning rate and batch size, both of which influence the performance in Spaced KD. 
We then perform extensive experiments to demonstrate the effectiveness of Spaced KD, across various benchmark datasets and network architectures. The proposed Spaced KD achieves strong performance gains (e.g., up to 2.31\% and 3.34\% on Tiny-ImageNet over regular KD methods of online KD and self KD, respectively) without additional training costs.
We further demonstrate the robustness of the space interval, the critical period of the spacing effect, and its plug-in nature to a broad range of advanced KD methods.
% (e.g., CIFAR-100 and Tiny-ImageNet) (e.g., ResNet and transformer based architectures)

Our contributions can be summarized as follows: (1) We draw inspirations from the paradigm of biological learning and propose to incorporate its spacing effect to improve online KD and self KD; (2) We theoretically analyze the potential benefits of the proposed spacing effect in terms of generalization, connecting it with the flatness of loss landscape; and (3) We conduct extensive experiments to demonstrate the effectiveness and generality of the proposed spacing effect across a variety of benchmark datasets, network architectures, and baseline methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
\section{Related Work}\label{sec:related}
\paragraph{Knowledge Distillation (KD).}
%KD was originally defined by Buciluǎ et al.~\citep{Modelcompression2006} and generalized by Hinton et al.~\citep{hinton2015distilling}. 
Representative avenues of KD can be generally classified into offline KD, online KD, and self KD, based on whether the teacher model is pre-trained and remains unchanged during the training process. Offline KD involves a one-way knowledge transfer in a two-phase training procedure. It primarily focuses on optimizing various aspects of knowledge transfer, such as designing the knowledge itself~\citep{hinton2015distilling,adriana2015fitnets}, and refining loss functions for feature matching or distribution alignment~\citep{huang2017like,asif2019ensemble,teacherassistant}.
%probabilisticKD,payingattention,li2020few,adversarialKD}
In contrast, online KD simplifies the KD process by training both teacher and student simultaneously and often outperforms offline KD. For instance, DML~\citep{deepMutual} implements bidirectional distillation between peer networks. %Further research explores balancing multiple teachers~\citep{multiTeacher} or teacher model construction~\citep{li2020explicit,liu2021semi,wu2021peer}. 
For self KD, the same network is used as both teacher and student~\citep{self-kd,das2023understanding,mobahi2020self,zhang2020self,yang2019snapshot,lee2019rethinking}.
%zhang2020self,yang2019snapshot,lee2019rethinking,phuong2019distillation,lan2019self,yuan2020revisiting}.
In this paper, the self KD we refer to is the distillation between different layers within the same network~\citep{self-kd,yan2024orchestrate,zhai2019lifelong}. 
%A recent work~\citep{allen2020towards} proved that it resembles an implicit ensembling strategy that improves test accuracy. 
However, existing methods for online KD and self KD often fail to effectively utilize high-capacity teachers over time, making it an intriguing topic to further explore the relationships between teacher and student models in these environments.

\paragraph{Adaptive Distillation.}\label{adaptiveKD}
Recent studies have found that the difference in model capacity between a much larger teacher network and a much smaller student network can limit distillation gains~\citep{KnowledgeGap_2020_CVPR,earlystop,liu2020search}. Current efforts to address this gap fall into two main categories: training paradigms~\citep{gao2018embarrassingly} and architectural adaptation~\citep{kang2020towards,gu2020search}. For instance, ESKD~\citep{earlystop} suggests stopping the training of the teacher early, while ATKD~\citep{mirzadeh2020improved} employs a medium-sized teacher assistant for sequential distillation. SHAKE~\citep{li2022shadow} introduces a shadow head as a proxy teacher for bidirectional distillation with students. However, existing methods usually implement adaptive distillation by adjusting teacher-student architecture from a spatial level. In contrast, Spaced KD provides an architecture- and algorithm-agnostic way to improve KD from a temporal level.

%is an architecture- and algorithm-agnostic method that opens a new direction for exploration from the temporal level.

\vspace{-.5em}
\paragraph{Flatness of Loss Landscape.}
The loss landscape around a parameterized solution has attracted great research attention~\citep{keskar2016large,hochreiter1994simplifying,izmailov2018averaging,dinh2017sharp,he2019asymmetric}. A prevailing hypothesis posits that the flatness of minima following network convergence significantly influences its generalization capabilities~\citep{keskar2016large}. In general, a flatter minima is associated with a lower generalization error, which provides greater resilience against perturbations along the loss landscape. This hypothesis has been empirically validated by studies such as~\cite{he2019asymmetric}. Advanced advancements have leveraged KD techniques to boost model generalization~\citep{deepMutual,zhao2023dot,self-kd}. Despite these remarkable advances, it remains a challenging endeavor to fully understand the impact of KD on generalization, especially in assessing the quality of knowledge transfer and the efficacy of teacher-student architectures.

\iffalse
\begin{figure*}
    %\vspace{-.1cm}
    \centering
    \includegraphics[width=0.8\linewidth]{./imgs/figs/fig2_Ucurve.pdf}
    \vspace{-.2cm}
    \caption{Alignment of spaced learning in BNNs and DNNs. \textbf{(a)} Computational cognitive model of spaced learning, modified from~\cite{landauer1969reinforcement}. \textbf{(b)} Overall performance of Spaced KD from different networks and benchmarks. R18: ResNet-18; R50: ResNet-50; R101: ResNet-101; C100: CIFAR-100; T200: Tiny-ImageNet. \textbf{(c)} Quadratic polynomial fitting of all performance from \textbf{(b)}. %All of them show that spaced training with appropriate interval can enhance learning performance. 
    %Depiction of the cognitive model was modified from~\cite{landauer1969reinforcement}.
    }
    \label{fig:main_result}
     \vspace{-.3cm}
\end{figure*}
\vspace{-.5em}
\paragraph{Spacing Effect in Biological Learning}
The spacing effect was first proposed by Ebbinghaus~\citep{ebbingham1913memory} in 1913. He conducted a self testing memory experiment by recalling lists of syllables and discovered that spacing repetitions over time was significantly more effective than cramming them all at once. Then, the superiority of spaced learning has been demonstrated across various species, including invertebrates (like \textit{Drosophila}~\citep{beck2000learning,pagani2009phosphatase}, bees~\citep{menzel2001massed}), rodents~\citep{anderson2008spaced,bello2013differential} and non-human primates~\citep{medin1974comparative,robbins1973memory}, as well as in various aspects of human learning, such as skill and motor learning~\citep{donovan1999meta,shea2000spacing}, classroom education~\citep{gluckman2014spacing,roediger2008learning,sobel2011spacing}, and the generalization of conceptual knowledge in children~\citep{vlach2014spacing}. To explain its effectiveness, Landauer~\citep{landauer1969reinforcement} was the first to propose and describe the dynamics of spaced learning (Fig.~\ref{fig:main_result}), such as the saturation of memory formation with mass trials, the failure of reinforcement with overly long trial spacing, and the way these factors contribute to creating an optimal inter-trial interval. Subsequently, some cognitive computing models~\citep{peterson1966short,wickelgren1972trace} inferred that a memory trace becomes not only stronger but also highly resistant to decay following spaced trials.
Inspired by this, we hypothesize that there may exist an optimal interval between the learning paces of teacher and student to improve the generalization of DNNs.
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Method
\section{Preliminaries}\label{sec:method} %\hw{citations...}

In this section, we first present the problem setup and some necessary preliminaries of KD. Then we describe the spacing effect in biological learning and discuss how it may inspire the design of KD.

\subsection{Problem Setup}\label{sec:kd_pre}

We describe the problem setup with supervised learning of classification tasks as an example.
Given $N$ training samples $\mathcal{D}_{\text{train}} = \{(x_i, y_i)\}_{i=1}^N$ where $x_i\in \mathbb{R}^d$ and $y_i\in \mathbb{R}^c$, the neural network model $f_{\theta}(\cdot): \mathbb{R}^d\mapsto \mathbb{R}^c$ with parameters $\theta\in \mathbb{R}^p$ is optimized by minimizing the empirical risk over $\mathcal{D}_{\text{train}}$ and evaluated over the test dataset $\mathcal{D}_{\text{test}}$. Using the SGD optimizer~\citep{sutskever2013importance}, $f_{\theta}(\cdot)$ is updated for each mini-batch of training data $\mathcal{B}_t = \{(x_i, y_i)\in \mathcal{D}_{\text{train}}\}_{i\in \mathcal{I}_t}$, $\mathcal{I}_t\subseteq \{1, 2, \cdots N\}$:
\begin{small}
\begin{equation}\label{eq:GD}
    \theta_{t+1} = \theta_t - \frac{\eta}{B}\sum_{i\in \mathcal{I}_t}\nabla_\theta L_i(\theta_t),
\end{equation}
\end{small}
where $L_i(\theta) = l_{\text{task}}(f_{\theta}(x_i), y_i)$ is a task-specific supervision loss. $\eta$ and $B = |\mathcal{I}_t|$ denote the learning rate and batch size, respectively. KD supports various kinds of interaction between multiple neural networks. The teacher-student framework we refer to here consists by default of a teacher network $g_{\phi}(\cdot)$ and a student network $f_{\theta}(\cdot)$, where the flow of knowledge transfer is often one-direction: the learning of $f$ is guided by the output of $g$, but not vice versa. The loss of student network $f$ in KD is bi-component as a weighted sum of task-specific and distillation loss ($l_{\text{task}}$ and $l_{\text{KD}}$), where a hyperparameter $\alpha$ controls the impact of teacher guidance:
\begin{small}
\begin{equation}\label{eq:loss_kd}
    L^\text{(KD)}_i (\theta, \phi) = (1-\alpha) l_{\text{task}}(f_{\theta}(x_i), y_i) + \alpha l_{\text{KD}}(f_{\theta}(x_i), g_{\phi}(x_i)).
\end{equation}
\end{small}
In many applications, the teacher network $g$ is often different from and much larger than the student network to obtain a more compact model. Meanwhile, there is an increasing number of efforts to implement KD to improve generalization for one particular architecture, where the teacher and student may share a common framework but differ in the random seeds for initialization. Some KD methods even treat different parts within one single network as teacher and student. Below we describe two representative methods:
% \ly{emphasize if the teacher and the student should use the same random seeds (for initialization and / or training dynamics)}
\paragraph{Online KD.} Though traditional KD assumes the teacher network $g$ as a pre-trained and powerful model, there exist scenarios where obtaining such a teacher is costly or impractical. 
% \ly{add more details of online KD itself, e.g., its motivation, common implementation, main benefits...}
Online KD is proposed to learn from an on-the-fly teacher network, allowing for dynamic adaptation during student training. In online KD, the updating of $g$ is aligned with $f$ for every mini-batch $\mathcal{B}_{t}$ with $\mathcal{I}_t$ (see Alg.~\ref{alg:online kd} in Appendix~\ref{sec: pseudo}) \footnote{For clarity, we use the same notation $\eta$, $B$ and $l_{\text{task}}$ to describe the training of $g$ and $f$, although they may select different training algorithms and hyperparameter values in practice.
%Here we use the same $\eta$, $B$ and $l_{\text{task}}$ as student training for simplicity, though strictly speaking, the training algorithm and hyperparameter selection of teacher network are quite arbitrary and scenario-dependent.
}:
\begin{small}
\begin{equation}\label{eq: teacher}
    \begin{split}
        \phi_{t+1} &= \phi_t - \frac{\eta}{B}\sum_{i\in \mathcal{I}_t} \nabla_{\phi}L^{\text{(teacher)}}_i(\phi_t) \\
                   &= \phi_t - \frac{\eta}{B}\sum_{i\in \mathcal{I}_t} \nabla_{\phi}l_{\text{task}}(g_{\phi_t}(x_i), y_i).
    \end{split}
\end{equation}
\end{small}

The design of an online teacher is quite demand-oriented, it could be simply a copy of the student network~\citep{li2022distilling, wu2021peer}. But to maintain a valid knowledge gap between student and teacher, they are often initialized using different random seeds in practice. Besides, the training process of teacher network could also be intervened by auxiliary loss from students through reverse distillation~\citep{li2022shadow, qian2022switchable, shi2021follow}.

\paragraph{Self KD.}
As an alternate approach to a pre-trained teacher, self KD utilizes the hidden information within the student network to guide its learning process. Instead of relying on a large external model, self KD achieves multiple knowledge alignments by introducing auxiliary blocks or creating different representations of the same encoded data. For a block-wise network, $f_{\theta} = f_{\theta_1}\circ f_{\theta_2}\circ\cdots\circ f_{\theta_m}$ that is composed of $m$ consecutive modules, the whole network $f_\theta$ is regarded as teacher while shallower blocks $f_{\theta_{1\sim k}}=f_{\theta_1}\circ \cdots\circ f_{\theta_k}$ ($1\leq k< m$) are students. Following the common setting~\citep{self-kd}, $\theta$ is updated with multiple task supervision and cross-layer distillation, which in fact can be formulated in terms of $L^{\text{(teacher)}}$ in Eq.~\ref{eq: teacher} and $L^{\text{(KD)}}$ in Eq.~\ref{eq:loss_kd} (see Alg.~\ref{alg:self kd} in Appendix~\ref{sec: pseudo}):
\begin{small}
\begin{equation}
\label{eq: self-kd}
    \theta_{t+1} = \theta_t - \frac{\eta}{B}\sum_{i\in \mathcal{I}_t}\nabla_\theta\left[
    L_i^{\text{(teacher)}}(\theta) + 
    \sum_{k=1}^{m-1}L_i^{\text{(KD)}}(\theta_{1\sim k}, \theta)
    \right].
\end{equation}
\end{small}

% \paragraph{Self KD.} As an alternate approach to a pre-trained teacher, self KD utilizes the hidden information within the student network to guide its learning process. Instead of relying on a large external model, self KD achieves multiple knowledge alignments by introducing auxiliary blocks or creating different representations of the same encoded data. Although it is technically hard to draw the boundary between teacher and student in self KD, here we still keep the same notation $g$ and $f$ as that of general online KD \textcolor{darkgreen}{(see Alg.~\ref{alg:self kd} in Appendix~\ref{sec: pseudo}).}
%\textcolor{darkgreen}{presenting pseudo code Alg.~\ref{alg:self kd} of self KD  in~\ref{sec: pseudo} and the practical deployment in Sec.~\ref{sec:exp_settings}.}

\subsection{Spacing Effect in Biological Learning}

%\paragraph{Biological Basis} 

Originally discovered by ~\cite{ebbingham1913memory}, the biological spacing effect highlights that the distribution of study sessions across time is critical for memory formation. %The biological spacing effect was initially discovered by~\cite{ebbingham1913memory} and highlighted 
Then, its functions have been widely demonstrated in various animals and even humans (see Sec.~\ref{sec:intro}).
% animal~\citep{pagani2009phosphatase,menzel2001massed,anderson2008spaced,medin1974comparative} and human~\citep{donovan1999meta,shea2000spacing,gluckman2014spacing,roediger2008learning,sobel2011spacing,vlach2014spacing}.
Many cognitive computing models have proposed the concept of spaced learning and described its dynamics, positing an optimal inter-trial interval during memory formation~\citep{landauer1969reinforcement,peterson1966short,wickelgren1972trace}. These studies motivate us to further investigate if a proper space interval could benefit KD of possible data variability across training batches. Here we provide more detailed explanations of the interdisciplinary connections:
%Therefore, we assume that a proper space interval could benefit the adaptation of possible data variability across batches for distillation.
% We hypothesize the proper space interval will allow for adapting to the potential variability of the data across batches in the distillation.
% \sgl{Encoding variability is a well-known cognitive theory to explain the superiority of spaced learning, in which the increasing variability of the environment during learning could improve the generalization and robustness.}
%\ly{maybe add some brief descriptions of its mechanisms, demonstrating that it is an innate property of BNNs from evolution, similar to the innate property of Spaced KD in DNNs.}

%In machine learning, KD is essentially a strategy to optimize the parameters of a student network with a teacher network by regularizing their outputs to be consistent in response to similar inputs. As shown in a recent work~\citep{allen2020towards}, KD shares a common mechanism as ensemble learning (EL) in improving generalization, and self KD is equivalent to an implicit combination of KD and EL. In comparison, the biological spacing effect can also be generalized to a kind of EL at the \emph{temporal} scale, as the brain network processes similar inputs with a certain time interval and updates its synaptic weights based on previous synaptic weights, which allows for stronger learning performance at test time~\citep{pagani2009phosphatase,smolen2016right}.
In machine learning, KD aims to optimize the parameters of a student network with the help of a teacher network by regularizing their outputs to be consistent in response to similar inputs. As shown in a pioneering theoretical analysis~\citep{allen2020towards}, KD shares a similar mechanism with ensemble learning (EL) in improving generalization from the training set to the test set. In particular, online KD performs this mechanism at temporal scales, and self KD can be seen as a special case of online KD.
In comparison, the biological spacing effect can also be generalized to a kind of EL at temporal scales, as the brain network processes similar inputs with a certain time interval and updates its synaptic weights based on previous synaptic weights, which allows for stronger learning performance at test time~\citep{pagani2009phosphatase,smolen2016right}. 


%Therefore, we propose Spaced KD by introducing the bio-inspired spacing effect on the basis of the underlying connections between KD and temporal EL. 
The proposed Spaced KD draws inspirations from the biological spacing effect and capitalizes on the underlying connections between KD and EL.
It incorporates a space interval between teacher and student to improve generalization. In particular, we hypothesize that an optimal interval may exist between the learning paces of teacher and student in DNNs, as in BNNs.

%, validated via both theoretical and empirical analyses. Inspired by this, we hypothesize that there may exist an optimal interval between the learning paces of teacher and student to improve the generalization of DNNs.

\begin{figure*}
    %\vspace{-.1cm}
    \centering
    \includegraphics[width=0.8\linewidth]{./imgs/figs/fig2_Ucurve.pdf}
    \vspace{-.2cm}
    \caption{Alignment of spaced learning in BNNs and DNNs. \textbf{(a)} Computational cognitive model of spaced learning, modified from~\cite{landauer1969reinforcement}. \textbf{(b)} Overall performance of Spaced KD from different networks and benchmarks. R18: ResNet-18; R50: ResNet-50; R101: ResNet-101; C100: CIFAR-100; T200: Tiny-ImageNet. \textbf{(c)} Quadratic polynomial fitting of all performance from \textbf{(b)}. %All of them show that spaced training with appropriate interval can enhance learning performance. 
    %Depiction of the cognitive model was modified from~\cite{landauer1969reinforcement}.
    }
    \label{fig:main_result}
     \vspace{-.3cm}
\end{figure*}


\section{Spaced KD}
In this section, we describe how Spaced KD is implemented into online KD and self KD, and include a pseudo code for each in Appendix~\ref{sec: pseudo}. We then theoretically analyze the benefit of the proposed spacing effect in improving generalization.

\subsection{Incorporating Spacing Effect into KD}\label{sec:spaced_kd}

By applying spaced learning in the pipeline of KD, more precisely in the context of online KD, we implement a process of alternate learning between teacher and student. The teacher network updates itself several steps in advance, and then it helps the student network train on the same set of batches. Formally, we define a hyperparameter \textit{Space Interval} denoted as $s$ to represent the gap between the teacher's and student's learning pace. Spaced KD is described as follows (see Fig.~\ref{fig:diagram}): 
% \ly{emphasize that self KD is a special case of online KD}
\begin{enumerate}
    \item First, we train the teacher $g_{\phi_t}(\cdot)$ for $s$ steps (from $\mathcal{B}_{t}$ to $\mathcal{B}_{t+s-1}$) according to the learning rule in Eq.~\ref{eq: teacher}, obtaining an advanced teacher $g_{\phi_{t+s}}(\cdot)$ identical to that of online KD.
    \item Then, we freeze the parameters $\phi_{t+s}$ of our teacher $g$, and start to transfer knowledge from it to the student $f_{\theta_{t}}(\cdot)$ that lags behind over the same batches of training data $\mathcal{B}_{t\sim t+s-1}$:
    \begin{small}
    \begin{equation}\label{eq:skd_in_general}
        \theta_{t+s} = \theta_t - \frac{\eta}{B} \sum_{j=t}^{t+s-1}\sum_{i\in \mathcal{I}_{j}}\nabla_{\theta} L_{i}^{\text{(KD)}}(\theta_j, \phi_{t+s}),
    \end{equation}
    \end{small}
    where $L_{i}^{\text{(KD)}}$ is the same as Eq.~\ref{eq:loss_kd} but using fixed teacher parameters $\phi_{t+s}$.
\end{enumerate}

Intrinsically, Spaced KD is a special case of online KD. The main difference that sets Spaced KD apart from online KD is the less frequent updates of the teacher network, which provides a relatively stable learning standard for the student network and potentially contributes to its better generalization ability than the online setting. In practice, we initialize the teacher in Spaced KD using the same random seed as the student. To take a closer look, we theoretically illustrate the impact of the proposed spacing effect on KD with step-by-step mathematical derivations in the next section.

\subsection{Theoretical Analysis}\label{sec:kd_theory}
% \ly{add the extended explanation of our rebuttal: (1) the information direction on the top of random direction of SGD; (2) why offline KD is theoretically desirable but not applicable in practice; (3) why online KD is sub-optimal in space interval}
To understand why Spaced KD might provide better generalization than online KD\footnote{For all theoretical analysis and conclusions in this section, we treat self KD as a special case of online KD since they share the same teacher-student relations. In the later Sec.~\ref{sec: results}, our experiments empirically support this argument as they behave similarly.}, we analyze the \textit{Hessian matrix} of the loss function for the student network in both scenarios. The Hessian matrix plays a crucial role in understanding the curvature of the loss landscape. In literature, various metrics related to the Hessian matrix have been adopted to evaluate the flatness of a loss minimum after training convergence, reflecting the generalization ability of the trained model~\citep{cifar100, blanc2020implicit, damian2021label, zhou2020towards}. Here we choose the Hessian trace as a representative for convenience. A smaller Hessian trace indicates a flatter loss landscape, which has also been proved to be related to the upper bound of test set generalization error. %~\citep{tsuzuku2020normalized, blanc2020implicit, damian2021label, orvieto2023explicit}.
\vspace*{-1em}
\paragraph{Setup.} For simplicity we set the dimension of class space as $c=1$, and the extension of $c>1$ is straightforward. Let the mean square error (MSE) be the task-specific loss. The KD loss characterizes the distance between two distributions $\hat{y}$ and $y$: $l_{\text{task}}(\hat{y}, y) = l_{\text{KD}}(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2$.
\vspace*{-1em}
\paragraph{Hessian Matrix.} For KD loss at the $i$-th data sample that follows Eq.~\ref{eq:loss_kd}, the Hessian matrix at a point $\theta$ of student $f_{\theta}(\cdot)$ with respect to its teacher $g_{\phi}(\cdot)$ can be calculated as the second-derivative of the empirical risk $L^{\text{(KD)}}(\theta, \phi) = \frac{1}{N}\sum_{i=1}^N L_{i}^{\text{(KD)}}(\theta, \phi)$. It could be easily verified that:
\begin{small}
\begin{equation}\label{eq:hessian_kd}
    \begin{split}
      H_{\phi}(\theta) &= \nabla_\theta^2 L^{\text{(KD)}}(\theta, \phi) \\
      &= \frac{1}{N}\sum_{i=1}^N \left[\nabla_\theta f_{\theta}(x_i)\nabla_\theta f_{\theta}(x_i)^\top + \beta(i, \theta, \phi)\nabla_\theta^2 f_{\theta}(x_i)\right],      
    \end{split}
\end{equation}
\end{small}
where $\beta(i, \theta, \phi) = (1-\alpha)(f_{\theta}(x_i) - y_i) + \alpha(f_{\theta}(x_i) - g_{\phi}(x_i))$, and in fact $\nabla_\theta L_i^{\text{(KD)}}(\theta, \phi) = \beta(i, \theta, \phi)\nabla_\theta f_\theta(x_i)$. At arbitrary time stamp $t$ during the supervised training process, the teacher model's parameters for student $\theta_t$ in online KD is $\phi_t$. In Spaced KD it should be $\phi_{k(t)}$ with $k(t) = (\lceil t/s \rceil)s$ where $\lceil\cdot\rceil$ denotes ceiling operation. Notice that for online KD, the loss function constantly changes due to the update of the teacher, but when we focus on the loss curve for a particular $\phi$, the differentiability of $L_{i}^{\text{(KD)}}$ are preserved, allowing us to continue the discussion.

\begin{definition}[Local linearization.]\label{def:local_lienar}
Let $\theta^*$ be a local minimizer of loss function w.r.t $f_{\theta}(\cdot)$, we call the local linearization of $f_{\theta}(\cdot)$ at $\theta$ around $\theta^*$ as:
$f_{\theta}(x) = f_{\theta^*}(x) + \langle\theta - \theta^*, \nabla_\theta f_{\theta^*}(x)\rangle$.
\end{definition}
\vspace*{-1em}
For both teacher and student networks, this linearized model in Def.~\ref{def:local_lienar} provides an applicable approximation of the local dynamic behavior around a converged point. We denote $\phi^*$ and $\theta^*$ as the local minimizer of teacher and student, respectively. Without loss of generality, we assume that after enough learning steps, $\forall x_i$, $g_{\phi^*}(x_i) = f_{\theta^*}(x_i) = y_i$ which means both models follow the over-parameterized setting so that their training set accuracy eventually become 100\%. Therefore, when the student network $f_{\theta}(\cdot)$ converges to a local minimizer $\theta^*$ in both online KD and Spaced KD, its corresponding teacher network $g_{\phi}(\cdot)$ should also be close to $\phi^*$ :
\begin{small}
\begin{equation}\label{eq:beta_optim}
\begin{split}
\beta(i, \theta^*, \phi) &= (1-\alpha)(f_{\theta^*}(x_i) - y_i) + \alpha(f_{\theta^*}(x_i) - g_{\phi}(x_i)) \\
& = \alpha \langle \phi - \phi^*, \nabla_\phi g_{\phi^*}(x_i) \rangle = \alpha \Delta \phi^\top \nabla_\phi g_{\phi^*}(x_i),
\end{split}
\end{equation}
\end{small}
where $\Delta \phi = \phi - \phi^*$. $\beta$ directly reflects the difference in the teacher model updating between online KD and Spaced KD. We then demonstrate how the combination of mini-batch training and space interval affects the role of the teacher model under the KD framework.

\begin{definition}[Teacher model gap]
For a teacher model $g_{\phi}(\cdot)$ trained with SGD using the updating rule in Eq.~\ref{eq: teacher}, we define current prediction error over training dataset as the performance gap between $\phi$ and loss minima $\phi^*$: $u(\phi) = \frac{1}{N}\sum_{i=1}^N |\Delta \phi^\top\nabla_{\phi}g_{\phi^*}(x_i)|$.
\end{definition}
At a training step $t$ close to convergence (a global time stamp) of the student model, considering the randomness brought by mini-batch sampling, we denote $u_t =\mathbb{E}[u(\phi_t)]$ for online KD, and $u_{k(t)} = \mathbb{E}\left[u\left(\phi_{k(t)}\right)\right]$ for Spaced KD (with space interval $s$) as the parameter gap of their corresponding teacher models, respectively.

% \ly{add more explanations why spaced KD is better than online KD (discuss the impact of $s$)}
\begin{lemma}[Lower risk of spaced teacher]\label{lem: low_risk}
$u_{k(t)} \leq u_t$.
\end{lemma}
\vspace*{-1em}
\begin{proof}
It is straightforward that the teacher with $\phi_{k(t)}$ in Spaced KD is an advanced model which has undergone several updating iterations ahead of the student at step $t$. Namely, by definition $t\leq k(t)=(\lceil t/s \rceil)s \leq t+s$. Thus, given the fact that SGD eventually selects a loss minima with linear stability~\citep{wu_alignment_2022}, i.e., $\mathbb{E}[L^{\text{(teacher)}}(\phi_{t+1})]\leq \mathbb{E}[L^{\text{(teacher)}}(\phi_{t})]$ around $\phi^*$, we have $u_{k(t)}\leq u_t$.
\end{proof}
% \vspace*{-1em}
\begin{theorem}\label{thm: hessian_trace_online_vs_spaced}
If the student model $f_{\theta}(\cdot)$ converges to a local minimizer $\theta^*$ at step $t$ of SGD, let $H_{\phi_t}^{\text{(O)}}(\theta^*)$ and $H_{\phi_k}^{\text{(S)}}(\theta^*)$ be the Hessian of online KD and Spaced KD, then
$$\mathbb{E}[\text{Tr}(H_{\phi_k}^{(S)}(\theta^*))]\leq \mathbb{E}[\text{Tr}(H_{\phi_t}^{(O)}(\theta^*))].$$
\end{theorem}
\vspace*{-.5em}
The comparison between the Hessian trace for Spaced KD and online KD finally settles in the difference between a spaced but advanced teacher and a frequently updated teacher. Detailed proof of Theorem~\ref{thm: hessian_trace_online_vs_spaced} are provided in Appendix~\ref{sec: thm_pf} with the help of Lemma~\ref{lem: low_risk}, indicating a flatter loss landscape and thus potentially better generalization ability for the student network of Spaced KD.
\vspace*{-1em}
\paragraph{Discussion.}
The above analysis reveals key distinctions between Spaced KD, offline KD, and online KD. Spaced KD guides the student $f$ with a well-defined trajectory established by the teacher $g$ that is slightly ahead in training~\cite{shi2021follow, rezagholizadeh2021pro}, thereby ensuring low errors along such informative direction to improve generalization. 
With an ideal condition where $g$ and $f$ converge to the same local minima, offline KD and Spaced KD should perform identically best. However, this ideal condition hardly exists in practice, especially given the nature of over-parameterization in advanced DNNs and the complexity of real-world data distributions. These two challenges result in a highly non-convex loss landscape of both $g$ and $f$ with a large number of local minima. Therefore, using a well-trained teacher in offline KD tends to be sub-optimal since $g$ and $f$ can easily converge to different local minima with SGD.
In comparison, the limitation of online KD lies in its narrow, constant interval between $g$ and $f$, restricting the exploration of informative directions. By maintaining an appropriate spaced interval, Spaced KD allows for broader explorations and encourages convergence to a more desirable region of the loss landscape, empirically validated in the following section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experimental Results
\section{Experiment}\label{sec: results}
In this section, we first describe experimental setups and then present experimental results.
%In this section, we first evaluate the spacing effect on typical architectures and datasets under the online KD and self KD frameworks in Sec.~\ref{sec: self_vs_online}. Then, in Sec.~\ref{sec: critical} we analyze the critical periods during training when the spacing effect affects performance. Next, we conduct several validation experiments to examine the generalization characteristics of the student network trained with Spaced KD in Sec.~\ref{sec: generalization} and the influence of hyperparameters (e.g., loss function, learning rate and batch size)  in Sec.~\ref{sec: hyperp}.
% \ly{If you want to save some space: In this section, we first describe the experimental setups and then present the experimental results with an in-depth analysis.}

\subsection{Experimental Setups}\label{sec:exp_settings}

\paragraph{Benchmark.} We evaluate the proposed spacing effect on both ResNet-based architectures~\citep{resnet} such as ResNet-18, ResNet-50 and ResNet-101, and transformer-based architectures~\citep{vit} such as DeiT-Tiny~\citep{touvron2021training} and PiT-Tiny~\citep{heo2021rethinking}. We consider four commonly used image classification datasets: CIFAR-100~\citep{cifar100}, Tiny-ImageNet, ImageNet-100, and ImageNet-1K~\citep{imagenet}. CIFAR-100 is a well-known image classification dataset of 100 classes and the image size is $32\times32$. Tiny-ImageNet consists of 200 classes and the image size is $64\times64$. ImageNet-100 and ImageNet-1K contain 100 and 1000 classes of images, respectively, and the image size is $224\times224$. 

% \vspace*{-1em}
\paragraph{Implementation.} For ResNet-based architectures, we use an SGD optimizer~\citep{sutskever2013importance} with 0.9 momentum, 128 batch size, 80 epochs, and a constant learning rate of 0.01.
%To avoid the influence of the learning rate, we used a constant learning rate of 0.01. 
For KD-related hyperparameters~\citep{self-kd}, we use a distillation temperature of 3.0, a feature loss coefficient of 0.03, and a KL-Divergence loss weight of 0.3. For transformer-based architectures, we use an AdamW optimizer~\citep{loshchilov2017decoupled} of batch size 128 and epoch number 300 (warm-up for 20 epochs). Besides, a cosine learning rate decay policy~\citep{loshchilov2017sgdr} is utilized with initial learning rate $5e-4$ and final $5e-6$, following the training pipeline of previous works~\citep{liu2021efficient, li2022locality, sun2024logit}. %We train all transformer-based architectures on up-sampled images with a resolution of $224 \times 224$ and set the weight of distillation loss to $0.5$.

For Spaced KD, we manually control a sparse interval $s$ in terms of epochs, which is proportional to the total number of samples in the training set (e.g., $s=0.5$ denotes half of the training set). To avoid potential bias, the training set is shuffled and both teacher and student receive the same data flow.
In online KD, the teacher employs the same network architecture as the student if not specified, distilling both response-based~\citep{hinton2015distilling} and feature-based~\citep{adriana2015fitnets} knowledge. 
In self KD, the teacher is the deepest layer of the network and the students are the shallow layers along with auxiliary classifiers~\citep{self-kd}. Specifically, ResNet-based architectures consist of 4 blocks so 3 students correspond to the three shallower blocks. The number of students for transformers depends on the network depth, namely, 11 in our setup. Auxiliary alignment layers and classifier heads are utilized to unify the dimensions of feature and logit vectors produced by students from different depths for distillation. Unless otherwise specified, all results are averaged over three repeats.

\subsection{Effectiveness and Generality of Spacing Effect} \label{sec: self_vs_online}

\paragraph{Overall Performance.}
% The results of experiments on CIFAR-100, Tiny-ImageNet, and ImageNet-100 are displayed in Tab.~\ref{table:main_results}. 
Our proposed Spaced KD outperforms traditional online KD (see Tab.~\ref{table:main_results}) and self KD (see Tab.~\ref{table:main_results_self}) across different datasets and networks. The performance of different intervals can be seen in Fig.~\ref{fig:main_result} and Tab.~\ref{table:extend_main}. Compared to vanilla online KD and self KD, the enhancement of accuracy is \textbf{2.14\%} on average, with moderate variations from a minimum of 1.19\% on ResNet-101 / CIFAR-100 to a maximum of 3.44\% on ResNet-101 / Tiny-ImageNet. For the larger dataset ImageNet-1K, our Spaced KD improves the performance for ResNet-18 and ViT networks by up to 5.08\% (see Tab.~\ref{table:imagenet_resnet}, Tab.~\ref{table:imagenet} of Appendix~\ref{sec: imagenet}).
% \vspace{-0.05cm}
\paragraph{Teacher-Student Gap.} Considering that capacity gaps between teacher and student for their different architectures or training progress would affect distillation gains (see Sec.~\ref{adaptiveKD}), we further evaluate various teacher-student pairs across model sizes and architectures, and Spaced KD remains effective in all cases (see Tab.~\ref{table:bigDsmall} and Tab.~\ref{table:offline} in Appendix~\ref{sec: big_small}).
Interestingly, if we train the teacher ahead of the student by $s$ steps at the beginning and then distill its knowledge to the student maintaining a constant training gap, there is no significant improvement over the online KD (see Tab.~\ref{table:time}). This indicates the particular strength of Spaced KD, which applies in the later stage rather than the early stage. 

%\sgl{This indicates that the spacing effect applies in the later stage of training rather than the early stage. %a ahead teacher of the student is more important than a "stable" teacher, and } \ly{This indicates that / This is because ...}
%As shown in Table~\ref{table:time}, there is no significant improvement over the online KD.


% A major advantage of Spaced KD is its straightforward implementation, needing only the addition of an interval in an online manner. In contrast, traditional distillation methods require designing and training an over-parameterized teacher model or developing a complex teacher-student interaction framework. Building a high-quality teacher model demands extensive experimentation to optimize its hyperparameters or even datasets for pre-training. Spaced KD avoids these challenges, being both architecture- and algorithm-agnostic, which is easily integrated into many widely used pipelines.

\begin{table}[t]
	\centering
    % \vspace{-0.1cm}
    \caption{Overall performance of online KD (\%).
    % Online KD distills knowledge from a teacher model to a student model with the same architecture, utilizing both response-based~\citep{hinton2015distilling} and feature-based~\citep{adriana2015fitnets} knowledge. Self KD~\citep{self-kd} distills knowledge from the deep layer to the shallow layer of the same network.
    Here are the results for online KD with an interval of 1.5 epochs. The performance of different intervals can be seen in Fig.~\ref{fig:main_result} and Tab.~\ref{table:extend_main}. $\Delta$ indicates Spaced KD's performance gain w.r.t online KD. %Due to limited computational resources, the results for transformer-based architectures are recorded for 1 run.
    } 
      % \vspace{0.1cm}
	\smallskip
      \renewcommand\arraystretch{1.2}
    \small{
	\resizebox{0.49\textwidth}{!}{ 
        % \begin{tabular}{cccccccccc}
        % \multirow{2}{*}{Network} & \multicolumn{3}{c}{CIFAR100}    & \multicolumn{3}{c}{Tiny-ImageNet} & \multicolumn{3}{c}{ImageNet-100} \\
        %                          & w/o KD & w/ KD & w/ Ours        & w/o KD  & w/ KD  & w/ Ours        & w/o KD    & w/ KD    & w/ Ours   \\
        % resnet 18                & 68.12  & 71.05 & \textbf{72.43} & 53.08   & 59.19  & \textbf{60.75} & 77.72     & 78.73    & 80.39     \\
        % resnet 50                & 69.62  & 71.85 & \textbf{73.77} & 56.41   & 60.99  & \textbf{63.30} & 77.95     & 79.78    & 82.43     \\
        % resnet 101               & 70.04  & 72.03 & \textbf{73.22} & 56.99   & 61.29  & \textbf{63.76} & -         & -        & -         \\
        % DeiT-Tiny                & 64.77  & 65.67 & \textbf{67.3}  & 50.23   & 51.82  & \textbf{54.2}  & 70.52     & 70.72    & 73.34     \\
        % PiT-Tiny                 & 73.45  & 74.14 & \textbf{75.55} & 57.89   & 58.25  & \textbf{60.25} & 76.1      & 76.6     & 78.34    
        % \end{tabular}
 
        \begin{tabular}{cccccc}
        % \hline
        \toprule
        \multirow{1}{*}{Dataset}       & Network                  & w/o KD                  & w/o Ours   & w/ Ours  & $\Delta$        \\ \hline
        \multirow{5}{*}{CIFAR-100}     & ResNet-18                & 68.12                   & 71.05      & \textbf{72.43}    & +1.38  \\
                                       & ResNet-50                & 69.62                   & 71.85      & \textbf{73.77}    & +1.92   \\
                                       & ResNet-101               & 70.04                   & 72.03      & \textbf{73.22}    & +1.19   \\
                                       & DeiT-Tiny                & 64.77                   & 65.67      & \textbf{67.30}    & +1.63  \\
                                       & PiT-Tiny                 & 73.45                   & 74.14      & \textbf{75.55}    & +1.41   \\ \hline
        \multirow{5}{*}{Tiny-ImageNet} & ResNet-18                & 53.08                   & 59.19      & \textbf{60.75}    & +1.56   \\
                                       & ResNet-50                & 56.41                   & 60.99      & \textbf{63.30}    & +2.31  \\
                                       & ResNet-101               & 56.99                   & 61.29      & \textbf{63.76}    & +2.47  \\
                                       & DeiT-Tiny                & 50.23                   & 51.82      & \textbf{54.20}    & +2.38  \\
                                       & PiT-Tiny                 & 57.89                   & 58.25      & \textbf{60.25}    & +2.00  \\  \hline
       \multirow{4}{*}{ImageNet-100}   & ResNet-18                & 77.82                   & 78.73      & \textbf{80.39}    & +1.66   \\
                                       & ResNet-50                & 77.95                   & 79.78      & \textbf{82.43}    & +2.65   \\
                                       & DeiT-Tiny                & 70.52                   & 70.72      & \textbf{73.34}    & +2.62  \\
                                       & PiT-Tiny                 & 76.10                   & 76.60      & \textbf{78.34}    & +1.74   \\ \toprule
                                       
        \end{tabular}
	} }
	\label{table:main_results}
	% \vspace{-0.05cm}
\end{table}

\begin{table}[t]
	\centering
    %\vspace{-0.1cm}
    \caption{Overall performance of self KD (\%).
    % Online KD distills knowledge from a teacher model to a student model with the same architecture, utilizing both response-based~\citep{hinton2015distilling} and feature-based~\citep{adriana2015fitnets} knowledge. Self KD~\citep{self-kd} distills knowledge from the deep layer to the shallow layer of the same network.
    Here are the results for self KD with an interval of 4.0 epochs. $\Delta$ indicates Spaced KD's performance gain w.r.t self KD.%Due to limited computational resources, the results for transformer-based architectures are recorded for 1 run.
    } 
      % \vspace{0.05cm}
	\smallskip
      \renewcommand\arraystretch{1.2}
    \small{
	\resizebox{0.49\textwidth}{!}{ 
        \begin{tabular}{cccccc}
        \toprule
        \multirow{1}{*}{Dataset}       & Network                  & w/o KD                    & w/o Ours  & w/ Ours           & $\Delta$    \\ \hline
        \multirow{3}{*}{CIFAR-100}     & ResNet-18                & 68.12                     & 73.29     & \textbf{75.73}    & +2.44 \\
                                       & ResNet-50                & 69.62                     & 75.73     & \textbf{78.73}    & +3.00    \\
                                       & ResNet-101               & 70.04                     & 76.16     & \textbf{79.24}    & +3.08   \\ 
                                       & Deit-Tiny                & 64.77                     & 65.24     & \textbf{68.26}    & +3.02   \\ \hline
        \multirow{4}{*}{Tiny-ImageNet} & ResNet-18                & 53.08                     & 61.08     & \textbf{62.83}    & +1.75  \\
                                       & ResNet-50                & 56.41                     & 63.58     & \textbf{65.80}    & +2.22  \\
                                       & ResNet-101               & 56.99                     & 63.35     & \textbf{66.79}    & +3.44  \\   
                                       & Deit-Tiny                & 50.17                     & 49.73     & \textbf{53.59}    & +3.86   \\ \hline
       \multirow{2}{*}{ImageNet-100}   & ResNet-18                & 77.82                     & 76.21     & \textbf{79.27}    & +3.06   \\  
                                       & Deit-Tiny                & 69.52                     & 70.50     & \textbf{73.46}    & +2.96   \\ \toprule
        \end{tabular}
	} }
	\label{table:main_results_self}
	% \vspace{-0.05cm}
\end{table}



\paragraph{Different KD Losses.}\label{sec:loss}
To evaluate generality, we implement Spaced KD with representative loss functions, such as L1, smooth L1, MSE (reduction=mean), MSE (reduction=sum), and cross-entropy. As shown in Tab.~\ref{table:loss}, Spaced KD applies to different loss functions with consistent improvements.

\begin{table}[t]
% \begin{small}
\centering
    \vspace{-0.05cm}
    \caption{Performance of Spaced KD on ResNet-18 / CIFAR-100 using different loss functions. %Spaced KD still works using a variety of representative loss functions.
    } 
      \vspace{0.1cm}
	\smallskip
      \renewcommand\arraystretch{1.2}
    \small{
    \resizebox{0.34\textwidth}{!}{ 
        \begin{tabular}{cccc}
        \toprule
        Loss Function & w/o & w/ $s=1.5$ & $\Delta$ \\ \hline
        L1           & \textbf{69.54}  & 69.30           & -0.24\\
        Smooth L1    & 68.96           & \textbf{69.45}  & +0.49\\
        MSE (Mean)   & 69.34           & \textbf{69.45}  & +0.11\\
        MSE (Sum)    & 71.05           & \textbf{72.43}  & +1.38\\
        Cross-Entropy & 70.38          & \textbf{72.04}  & +1.66\\ \toprule
        \end{tabular}
	} }
	\label{table:loss}
% \end{small}
	% \vspace{-8pt}
\end{table}


% \vspace{-0.1cm}
\paragraph{Different KD Methods.}\label{sec:sota}
%In addition to validate the compatibility of other KD methods, 
We combine our Spaced KD with other more advanced KD methods, including (1) TSB~\cite{li2022online} constructs superior "teachers" with temporal accumulator and spatial integrator; (2) CTKD~\cite{li2023curriculum} controls the task difficulty level during the student’s learning career through a dynamic and learnable temperature; (3) LSKD~\cite{sun2024logit} employs a plug-and-play Z-score pre-process of logit standardization before applying softmax and KL divergence.
% (1) traditional KD such as BAN~\citep{furlanello2018born} and TAKD~\citep{mirzadeh2020improved}, (2) online KD such as DML~\citep{deepMutual} and SHAKE~\citep{li2022shadow}, and (3) self KD such as DLB~\citep{DLB} and PSKD~\citep{PSKD} (see Appendix~\ref{sec:sota_implementation} for details). 
As shown in Tab.~\ref{table:sota}, Spaced KD brings significant improvements to a wide range of KD methods. The above results suggest that the benefits of Spaced KD arise from the fundamental properties of parameter optimization in deep learning, consistent with our theoretical analysis in Sec.~\ref{thm: hessian_trace_online_vs_spaced}.

\begin{table}
% \begin{small}
\centering
    % \vspace{-0.2cm}
    \caption{Performance of Spaced KD on ResNet-18 / CIFAR-100 using more recent KD methods. % All experiments are conducted with ResNet-18 and CIFAR-100. Spaced KD can be applied to a wide variety of pipelines.
    } 
      \vspace{0.1cm}
	% \smallskip
      % \renewcommand\arraystretch{1.2}

	\resizebox{0.45\textwidth}{!}{ 
\begin{tabular}{llrrr}
\toprule
%\multirow{2}{*}{Dataset/Model} & \multirow{2}{*}{Method} & TSB & CTKD & LSKD \\ & & \cite{li2022online}  & \cite{li2023curriculum}  &\cite{sun2024logit} \\
\multirow{1}{*}{Dataset/Model} & \multirow{1}{*}{Method} & TSB & CTKD & LSKD \\
%& & \cite{li2022online}  & \cite{li2023curriculum}  &\cite{sun2024logit} \\
\midrule
\multirow{3}{*}{CIFAR-100/ResNet-18} 
& w/o KD & 67.65 & 67.86 & 67.94 \\
& w/ KD & 71.70 & 69.41 & 70.74 \\
& w/ Ours & \textbf{72.82} & \textbf{71.12} & \textbf{71.76} \\ \hline
% \addlinespace

\multirow{3}{*}{CIFAR-100/DeiT-Tiny} 
& w/o KD & 51.90 & 53.31 & 52.31 \\
& w/ KD & 52.63 & 54.20 & 52.93 \\
& w/ Ours & \textbf{55.47} & \textbf{54.72} & \textbf{53.83} \\ \hline
% \addlinespace

\multirow{3}{*}{Tiny-ImageNet/ResNet-18} 
& w/o KD & 55.21 & 53.03 & 54.05 \\
& w/ KD & 59.92 & 58.78 & 59.30 \\
& w/ Ours & \textbf{61.65} & \textbf{60.32} & \textbf{60.28} \\ \hline
\addlinespace
% 
\multirow{3}{*}{Tiny-ImageNet/DeiT-Tiny} 
& w/o KD & 40.29 & 40.82 & 39.65 \\
& w/ KD & 40.13 & 41.22 & 41.14 \\
& w/ Ours & \textbf{43.36} & \textbf{41.60} & \textbf{41.48} \\
\bottomrule
% \begin{tabular}{cccc}
% \toprule
% Method      & w/o KD & w/ KD & w/ Ours \\ \hline
% BAN~\citep{furlanello2018born}   & 56.75  & 60.56 & \textbf{61.83}    \\
% TAKD~\citep{mirzadeh2020improved}  & 61.37  & 61.82 & \textbf{63.48}    \\
% DML~\citep{deepMutual}   & 68.92  & 70.31 & \textbf{71.80}     \\
% SHAKE~\citep{li2022shadow} & 69.02  & 72.02 & \textbf{72.64}  \\
% DLB~\citep{DLB}   & 68.80  & 68.87 & \textbf{69.31}  \\
% PSKD~\citep{PSKD} & 74.92  & 75.20 & \textbf{75.38}  \\
% \toprule
\end{tabular}
	} 
	\label{table:sota}
% \end{small}
	\vspace{-0.01cm}
\end{table}

\subsection{Extended Analysis of Spacing Effect}

\paragraph{Sensitivity of Space Interval.}
Through extensive investigation (see Fig.~\ref{fig:main_result} and Tab.~\ref{table:extend_main} in Appendix~\ref{sec: extend_main}), the space interval $s$ is relatively insensitive and $s=1.5$ results in consistently strong improvements.
Therefore, we selected it as the default choice to obtain the performance of our Spaced KD in all comparisons. This property also largely avoids the computational cost and complexity of model optimization imposed by the new hyperparameter.

\paragraph{Critical Period of Spaced KD.}\label{sec: critical}
%The above results demonstrate the superiority of Spaced KD, but its underlying mechanisms requires further exploration. 
%To investigate the critical period during which Spaced KD is most effective, we conduct in-depth experiments. 
In order to better understand the underlying mechanisms of Spaced KD, we empirically investigate the critical period of implementing the proposed spacing effect.
As shown in Fig.~\ref{fig:time}, we control the start time of spaced distillation throughout the training process, and discover that initiating Spaced KD in the later stage of training is more beneficial than the early stage for performance improvements of the student network. This suggests that in KD, not only the interval between learning sessions but also the timing of spaced learning are important. Unlike previous understandings that attribute the KD efficacy to the knowledge capacity gap between the teacher and the student (where Spaced KD should be more effective in the early stage of training, see Sec.~\ref{adaptiveKD}), our results point out a novel direction for KD research from a temporal perspective. Specifically, the ``right time to learn'' is critical for the student, and the teacher could influence the student's convergence to a better solution by intervening during the later training stage.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1.0\linewidth]{./imgs/figs/fig3_time.pdf}
    % \vspace{0 pt}
    \caption{Impact of different initiating times of Spaced KD ($s=1.5$), which is introduced \textbf{(a)} for constant 10 training epochs or \textbf{(b)} till the end of training. %Spaced KD is effective at the later stage of training, rather than the early stage.
    }
    \label{fig:time}
    % \vspace{-.0cm}
\end{figure}

% \vspace{-.1cm}
\paragraph{Learning Rate and Batch Size.}
As described in previous works, the learning rate and batch size influence the endpoint curvature and the whole trajectory~\citep{EarlyPhase,lewkowycz2020large,xie2020diffusion}. The learning rate corresponds to the parameters' updating step length, and batch size would affect the total number of updating iterations which directly relates to the choice of space interval $s$. Therefore, we further validate the impact of learning rate and batch size. As shown in Fig.~\ref{fig:hyperparameter} of Appendix~\ref{sec:hyperparameter}, we summarize the results: (i) Spaced KD proves effective w.r.t naive online KD and self KD across different learning rates; (ii) Spaced KD exhibits its advantages when training with a relatively large batch size (greater than 64). These observations also align with previous research~\citep{Jastrzebski2019Sharpest, wu_alignment_2022} regarding a small batch size limiting the maximum spectral norm along the convergence path found by SGD from the beginning of training. 

\begin{table*} [h]
	\centering
    \vspace{-0.1cm}
    \caption{Comparison of accuracy under image corruption attack (\%). $\Delta$ indicates Spaced KD's performance gain w.r.t online KD. The intensity of noise is 1.0 and the results of other intensities (i.e., 3.0, 5.0) can be seen in Tab.~\ref{table:extend_attack} of Appendix.~\ref{sec: corrupt}.
    } 
    % \vspace{0.1cm}
	\smallskip
      \renewcommand\arraystretch{1.2}
    % \small{
	\resizebox{0.87\textwidth}{!}{ 
       \begin{tabular}{lcccccccccc}
        \toprule
        \multirow{2}{*}{Method} &\multirow{2}{*}{Attack} & \multicolumn{3}{c}{ResNet-18}        & \multicolumn{3}{c}{ResNet-50}        & \multicolumn{3}{c}{ResNet-101}       \\ \cline{3-11} 
                                & & w/o Ours & w/ Ours        & $\Delta$ & w/o Ours & w/ Ours        & $\Delta$ & w/o Ours & w/ Ours        & $\Delta$ \\ \hline
        \multirow{6}{*}{Online KD} 
        &\texttt{impulse\_noise}          & 52.92    & \textbf{54.15} & 1.23     & 55.41    & \textbf{57.19} & 1.78     & 55.66    & \textbf{57.17} & 1.51     \\
        &\texttt{zoom\_blur}              & 66.45    & \textbf{67.43} & 0.98     & 66.86    & \textbf{68.53} & 1.67     & 66.20     & \textbf{66.53} & 0.33     \\
        &\texttt{snow}                    & 57.28    & \textbf{59.05} & 1.77     & 59.19    & \textbf{59.55} & 0.36     & 57.82    & \textbf{58.38} & 0.56     \\
        &\texttt{frost}                   & 57.55    & \textbf{59.64} & 2.09     & \textbf{60.16}    & 59.93          & -0.23    & 58.57    & \textbf{59.63} & 1.06     \\
        &\texttt{jpeg\_compression}       & \textbf{41.12}    & 40.23          & -0.89    & 42.22    & \textbf{42.65} & 0.43     & 42.69    & \textbf{43.70}  & 1.01     \\
        &\texttt{brightness}              & 67.01    & \textbf{69.06} & 2.05     & 68.31    & \textbf{69.15} & 0.84     & 66.82    & \textbf{67.45} & 0.63     \\  \hline
        
        \multirow{6}{*}{Self KD} 
        &\texttt{impulse\_noise}          & 50.65 & \textbf{60.57} &9.92 & 62.18 & \textbf{71.57} &9.39 & 59.33 & \textbf{68.78} &9.45     \\
        &\texttt{zoom\_blur}              & 64.44 & \textbf{68.60} &4.16 & 68.03 & \textbf{72.13} &4.10 & 66.09 & \textbf{71.16} &5.07     \\
        &\texttt{snow}                    & 61.30 & \textbf{66.14} &4.84 & 64.72 & \textbf{69.65} &4.93 & 64.03 & \textbf{68.76} &4.73     \\
        &\texttt{frost}                   & 63.96 & \textbf{67.80} &3.84 & 66.36 & \textbf{71.81} &5.45 & 66.73 & \textbf{70.21} &3.48     \\
        &\texttt{jpeg\_compression}       & 30.99 & \textbf{34.67} &3.68 & 34.44 & \textbf{35.34} &0.90 & 33.64 & \textbf{34.76} &1.12     \\
        &\texttt{brightness}              & 73.18 & \textbf{75.92} &2.74 & 74.91 & \textbf{79.19} &4.28 & 75.10 & \textbf{78.91} &3.81     \\ 
                
        
        \toprule



        
        \end{tabular}
	} %}
	\label{table:attack}
	 \vspace{-0.1cm}
\end{table*}

\subsection{Generalization of Spaced KD}\label{sec: generalization}
\paragraph{Flat Minima.}
To verify whether Spaced KD could converge to a flat minima, we conduct experiments to observe the model robustness that reflects the flatness of loss landscape around convergence, following previous works~\citep{deepMutual,self-kd}. We first train ResNet-18/50/101 networks on CIFAR-100 with traditional online KD (\texttt{w/o}) and our Spaced KD (\texttt{w/1.5}, the interval is 1.5 epochs). Then Gaussian noise is added to the parameters of those models to evaluate their training loss and accuracy over the training set at various perturbation levels, which are plotted in Fig.~\ref{fig:noise}. The results show that the model trained with Spaced KD maintains a higher accuracy and lower loss deviations than naive KD under gradient noise level. Furthermore, after applying this interference, the training loss of the independent model significantly increases, whereas the loss of the Spaced KD model rises much less. These results suggest that the model with Spaced KD has found a much wider minima, which is likely to result in better generalization performance. 

\begin{figure}[htb]
   % \vspace{-0.2cm}
    \centering
    \includegraphics[width=0.95\linewidth]{./imgs/figs/fig4_noise.pdf}
    \vspace{-0.01cm}
    \caption{Impact of Gaussian noise on performance. 
    % Under the same noise perturbations, the network trained with Spaced KD exhibits lower loss changes and higher accuracy.
    }
    \label{fig:noise}
    \vspace{-0.1cm}
\end{figure}

% \vspace{-.5em}
\paragraph{Noise Robustness.}
In addition to manipulating network parameters, we conduct an extra experiment to evaluate the model's generalization ability to multiple transformations that create out-of-distribution images. Specifically, we apply 6 representative operations of image corruption~\citep{michaelis2019dragon}
% (i.e., \texttt{impulse\_noise}, \texttt{zoom\_blur}, \texttt{snow}, \texttt{frost}, \texttt{jpeg\_compression} and \texttt{brightness}, see their visualization in Fig.~\ref{fig: attack} of Appendix.~\ref{sec: corrupt}) 
to the images of the CIFAR-100 test set. The test accuracy at noise intensity 1.0 is recorded in Tab.~\ref{table:attack} and results of other intensity levels can be found in Tab.~\ref{table:extend_attack} of Appendix~\ref{sec: corrupt}. It is clear that in most cases with different corruption types and network architectures, our proposed Spaced KD helps the student network resist noise attacks, which reflects its superior robustness to unseen inference situations. Besides, we test robust accuracy using a representative adversarial attack method called BIM~\citep{kurakin2017adversarial}, and our Spaced KD is more robust across different architectures (see Tab.~\ref{table:adversarial} in Appendix~\ref{sec:adversarial}). The above results empirically offer evidence for the generalization promotion brought by the spacing effect.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
\section{Conclusion}\label{sec:conclusion}

In this paper, we present Spaced Knowledge Distillation (Spaced KD), a bio-inspired strategy that is simple yet effective for improving online KD and self KD. We theoretically demonstrate that the spaced teacher helps the student model converge to flatter local minima via SGD, resulting in better generalization. With extensive experiments,
%Thorough evaluations are performed on the image classification, 
Spaced KD achieves significant performance gains across a variety of benchmark datasets, network architectures and baseline methods, providing innovative insights into the learning paradigm of KD from a temporal perspective.
%Based on our insight into the paradigm of KD, Spaced KD achieves this goal by acting near convergence points to help the student model converge to flat minima.
%The key finding of our work is that Spaced KD is a more effective learning procedure to boost performance during KD. 
%While most previous research focuses on enhancing KD efficiency through adaptive strategies, our work provides mechanistic insight by organizing the temporal architecture of the learning paradigm. 
Since we also reveal a possible critical period of spacing effect and provide its potential theoretical implications in DNNs, our findings may offer computational inspirations for neuroscience. 
By exploring more effective spaced learning paradigms and investigating detailed neural mechanisms, our work is expected to facilitate a deeper understanding of both biological learning and machine learning. % and brain studies.

%\ly{Here we further analyze potential limitations and social impacts:}
Although our approach has achieved remarkable improvements, it also has potential \emph{limitations}: Our results suggest a relatively insensitive optimal interval ($s=1.5$) for Spaced KD, yet remain under-explored its theoretical foundation and an adaptive strategy for determining it.
%It does not confirm the existence of an optimal interval for Spaced KD or how to determine it. 
Additionally, our results indicate that the timing of Spaced KD is important. The effectiveness of adaptive adjusting the space interval and the timing of distillation remains to be validated and analyzed in subsequent research. In future work, we would actively explore the application of such spacing effect for a broader range of scenarios, such as curriculum learning, continual learning, and reinforcement learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Acknowledgements
%\input{SPACE/sec/acknowledgments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References
% \bibliographystyle{iclr2025_conference}
% 


\paragraph{Acknowledgment.}
This work was supported by the STI2030-Major Projects (2022ZD0204900 to Y.Z.), the NSFC Projects (Nos.~62406160 to L.W., 32021002 to Y.Z.), and National Science and Technology Major Project (2022ZD01163013 to B.L.). L.W. is also supported by the Postdoctoral Fellowship Program of CPSF under Grant Number GZB20230350 and the Shuimu Tsinghua Scholar.

\paragraph{Impact Statements.}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliographystyle{icml2025}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix
\clearpage
\onecolumn
\appendix

\section{Appendix} % / supplemental material
% \newpage

\subsection{Proof of Theorem~\ref{thm: hessian_trace_online_vs_spaced}}\label{sec: thm_pf}
\begin{proof}
For a general KD loss, we have the trace of its Hessian matrix at global minimizer $\theta^*$:
\begin{equation}\label{eq:hessian_global}
\begin{split}
    \text{Tr}\left(H_\phi(\theta^*)\right) &= \frac{1}{N}\sum_{i=1}^N \left[\|\nabla_\theta f_{\theta^*}(x_i)\|^2 + \beta(i, \theta^*, \phi)\text{Tr}\left(\nabla_\theta^2 f_{\theta^*}(x_i)\right)\right] \\
    & = \frac{1}{N}\sum_{i=1}^N \left[\|\nabla_\theta f_{\theta^*}(x_i)\|^2 + \alpha \Delta \phi^\top \nabla_\phi g_{\phi^*}(x_i)\text{Tr}\left(\nabla_\theta^2 f_{\theta^*}(x_i)\right)\right].
\end{split}
\end{equation}
For online KD and Spaced KD, the expectation of their Hessian trace should be:
\begin{equation}\label{eq: hessian_trace_online}
\mathbb{E}[\text{Tr}(H_{\phi_t}^{(O)}(\theta^*))] = \mathbb{E}_i[\|\nabla_\theta f_{\theta^*}(x_i)\|^2] + \frac{\alpha}{N}\sum_{i=1}^N \mathbb{E}\left[\Delta \phi_t^\top \nabla_\phi g_{\phi^*}(x_i)\text{Tr}\left(\nabla_\theta^2 f_{\theta^*}(x_i)\right)\right],
\end{equation}
\begin{equation}\label{eq: hessian_trace_spaced}
\mathbb{E}[\text{Tr}(H_{\phi_k}^{(S)}(\theta^*))] = \mathbb{E}_i[\|\nabla_\theta f_{\theta^*}(x_i)\|^2] + \frac{\alpha}{N}\sum_{i=1}^N \mathbb{E}\left[\Delta \phi_k^\top \nabla_\phi g_{\phi^*}(x_i)\text{Tr}\left(\nabla_\theta^2 f_{\theta^*}(x_i)\right)\right].
\end{equation}

By Lemma~\ref{lem: low_risk}, $\frac{1}{N}\sum_{i}\mathbb{E}[|\Delta \phi_k^\top \nabla_\phi g_{\phi^*}(x_i)|]\leq \frac{1}{N}\sum_{i}\mathbb{E}[|\Delta \phi_t^\top \nabla_\phi g_{\phi^*}(x_i)|]$,
$$
\frac{\alpha}{N}\sum_{i=1}^N \mathbb{E}\left[\Delta \phi_k^\top \nabla_\phi g_{\phi^*}(x_i)\text{Tr}\left(\nabla_\theta^2 f_{\theta^*}(x_i)\right)\right] \leq \frac{\alpha}{N}\sum_{i=1}^N \mathbb{E}\left[\Delta \phi_t^\top \nabla_\phi g_{\phi^*}(x_i)\text{Tr}\left(\nabla_\theta^2 f_{\theta^*}(x_i)\right)\right].
$$

Substituting the above inequality into Eq.~\ref{eq: hessian_trace_online} and Eq.~\ref{eq: hessian_trace_spaced} completes the proof.
\end{proof}

%\newpage
\subsection{Performance of Different Intervals for Online KD}\label{sec: extend_main}
\begin{table*}[h]
% \begin{small}
\centering
    % \vspace{-0.2cm}
    \caption{Overall performance of different intervals for Fig.~\ref{fig:main_result} and Tab.~\ref{table:main_results}.
    } 
      %\vspace{-0.1cm}
	\smallskip
      \renewcommand\arraystretch{1.3}
    \small{
	\resizebox{0.80\textwidth}{!}{ 
\begin{tabular}{ccccccccc}
\toprule
Dataset                        & Network   & Baseline & w/o   & w/0.5 & w/1.0 & w/1.5          & w/2.0 & w/max          \\ \hline
\multirow{5}{*}{CIFAR-100}     & ResNet-18  & 68.12   & 71.05 & 72.02 & 72.03 & \textbf{72.43} & 72.18 & 72.22          \\
                               & ResNet-50  & 69.62    & 71.85 & 73.39 & 73.25 & \textbf{73.77} & 73.28 & 73.35          \\
                               & ResNet-101 & 70.04    & 72.03 & 73.11 & 73.22 & 72.91          & 73.22 & \textbf{74.01} \\ 
                               & DeiT-Tiny  & 64.77    & 65.67 & 66.03 & 66.22 & \textbf{67.30}          & 66.45 & 65.69 \\ 
                               & PiT-Tiny   & 73.45    & 74.14 & \textbf{75.55} & 75.50 & 75.27          & 75.12 & 74.07 \\ 
                               \hline %\cline{2-9} 
\multirow{5}{*}{Tiny-ImageNet} & ResNet-18  & 53.08    & 59.19 & 59.62 & 59.68 & \textbf{60.75} & 59.52 & 59.34          \\
                               & ResNet-50  & 56.41    & 60.99 & 62.13 & 62.27 & \textbf{63.30} & 62.55 & 62.47          \\
                               & ResNet-101 & 56.99    & 61.29 & 62.70 & 62.64 & \textbf{63.76} & 62.80 & 63.10          \\
                               & DeiT-Tiny  & 50.23    & 51.82 & \textbf{54.20} & 53.55 & 52.92                   & 53.48 & 52.21 \\ 
                               & PiT-Tiny   & 57.89    & 58.25 & 59.45          & 59.77 & \textbf{60.25}          & 59.75 & 58.23 \\ 
                               \toprule
\end{tabular}
	} }
	\label{table:extend_main}
% \end{small}
	% \vspace{-8pt}
\end{table*}

\subsection{Implementation of SOTA methods with Spaced KD}\label{sec:sota_implementation}
For traditional KD methods (BAN~\citep{furlanello2018born}, TAKD~\citep{mirzadeh2020improved}) and online KD methods (DML~\citep{deepMutual} and SHAKE~\citep{li2022shadow}), we preserve their basic training frameworks for reproducing results in \texttt{w/o KD} (raw ResNet-18 training) and \texttt{KD} (ResNet-18 with the corresponding method) columns and delay the students' supervised learning and distillation by a space interval of 1.5 epochs for \texttt{w/ Ours}. For self KD methods (DLB~\citep{DLB} and PSKD~\citep{PSKD}), we initiate a student network identical to the teacher. We train the teacher model utilizing PSKD or DLB, and the student model is trained either online or in a spaced style with an interval of 1.5 epochs. Specifically, the results \texttt{w/o KD} of PSKD and DLB in Tab.~\ref{table:sota} are the performance of the teacher model, \texttt{w/ KD} is the performance of online students, and \texttt{w/ Ours} corresponds to spaced students. Because we follow the exact training pipeline (including learning rate scheduler, optimizer, and dataset transformation, etc) of those works when reproducing their results, which is different from that of Tab.~\ref{table:main_results} and Tab.~\ref{table:main_results_self}, the baselines without KD may be different.

% We combine our Spaced KD with other more advanced KD methods, including (1) traditional KD such as BAN~\citep{furlanello2018born} and TAKD~\citep{mirzadeh2020improved}, (2) online KD such as DML~\citep{deepMutual} and SHAKE~\citep{li2022shadow}, \textcolor{darkgreen}{and (3) self KD such as DLB~\citep{DLB} and PSKD~\citep{PSKD}}. As shown in Tab.~\ref{table:sota}, Spaced KD brings significant improvements to a wide range of KD methods. The above results suggest that the benefits of Spaced KD arise from the fundamental properties of parameter optimization in deep learning, consistent with our theoretical analysis in Sec.~\ref{thm: hessian_trace_online_vs_spaced}.

% \newpage
\subsection{Performance of Spaced KD on ImageNet-1k}\label{sec: imagenet}
%For the limitation of computational resources, we only train the model for 1 repeat. However, the improvements of Spaced KD are significant.

\begin{table*}[h]
    \vspace{-0.2cm}
\centering
    \caption{Performance of ResNet-18 on ImageNet-1k Dataset (space interval 1.5 epochs).
    } 
    	\label{table:imagenet_resnet}
        \vspace{+0.1cm}
    \small{
    \resizebox{0.50\textwidth}{!}{ 
    \begin{tabular}{ccccc}
    \toprule
    Epoch   (epoch)    & 20     & 40    & 60     & 80   \\ \hline
    online KD	       &44.35	&45.94	&46.62	 &48.59 \\
    online KD w/ Ours  &49.28	&51.35	&52.36	 &53.67  \\
    self KD	           &44.87	&46.78	&47.78	 &47.99   \\
    self KD w/ Ours	   &47.03	&49.94	&50.22	 &51.57   \\ \toprule
    \end{tabular}
    }
    }
	\vspace{-0.3cm}
\end{table*}

\begin{table*}[h]
    \vspace{-0.2cm}
\centering
    \caption{Performance of Deit-Tiny on ImageNet-1k Dataset (space interval 1.5 epochs).
    } 
    	\label{table:imagenet}
        \vspace{+0.1cm}
    \small{
    \resizebox{0.40\textwidth}{!}{ 
    \begin{tabular}{cccc}
    \toprule
    Epoch   (epoch)  & 100     & 200   & 300   \\ \hline
    online KD	      &58.18	&65.93	&72.04   \\
    online KD w/ Ours	&58.47	&66.54	&72.34   \\
    self KD	&58.81	&66.37	&72.39   \\
    self KD w/ Ours	&60.82	&67.27	&73.69 \\ \toprule
    \end{tabular}
    }
    }
	\vspace{-0.3cm}
\end{table*}


\subsection{Performance of Spaced KD on Different Teacher-Student Architectures}\label{sec: big_small}
\begin{table*}[h]
% \begin{small}
\centering
    \vspace{-0.2cm}
    \caption{Overall performance of student networks distilled from different teachers on CIFAR-100. We use ResNet-18 as the student.%(space interval 1.5 epochs)% Spaced KD ($s = 1.5$ epochs) is better than the baseline (i.e., the teacher itself) in a variety of teacher capacities (width, depth and architecture), indicating that our method is still effective when distilling across model sizes and architectures.
    } 
      %\vspace{-0.1cm}
	\smallskip
      \renewcommand\arraystretch{1.3}
    \small{
	\resizebox{0.60\textwidth}{!}{ 
\begin{tabular}{clccc}
\toprule
\multicolumn{2}{c}{Teacher}                   & Baseline     & Online KD        & Spaced KD      \\ \hline
\multirow{3}{*}{Width}        & ResNet-18$\times$2   & 69.40        & 71.77            & \textbf{72.77} \\ 
                              & ResNet-18$\times$4   & 70.75        & 72.17            & \textbf{73.11} \\
                              & ResNet-18$\times$8   & 70.77        & 72.03            & \textbf{73.52} \\ \hline  
\multirow{2}{*}{Depth}        & ResNet-50            & 69.21        & 72.18            & \textbf{73.49} \\
                              & ResNet-101           & 69.54        & 71.61            & \textbf{73.04}      \\ \hline  
\multirow{2}{*}{Architecture} & DeiT-Tiny            & 64.65        & 78.61            & \textbf{79.38} \\
                              & PiT-Tiny             & 73.78        & 77.13            & \textbf{78.77}      \\ \toprule
\end{tabular}
	} }
	\label{table:bigDsmall}
% \end{small}
	\vspace{-0.3cm}
\end{table*}

\begin{table*}[h!]
% \begin{small}
\centering
    % \vspace{-0.2cm}
    \caption{Comparison of Spaced KD and offline KD from different teacher-student pairs on CIFAR-100. We use ResNet-18 as the student. %Spaced KD ($s = 1.5$ epochs) is better than the offline KD in a variety of teacher capacities.
    % Overall performance of student networks distilled from different teachers on CIFAR-100. We use ResNet-18 as the student. Spaced KD ($s = 1.5$ epochs) is better than the offline KD in a variety of teacher capacities.
    } 
      %\vspace{-0.1cm}
	\smallskip
      \renewcommand\arraystretch{1.3}
    \small{
	\resizebox{0.50\textwidth}{!}{ 
\begin{tabular}{clcccc}
\toprule
\multicolumn{2}{c}{Teacher}                           & Offline KD     & Spaced KD      \\ \hline
\multirow{3}{*}{Size}           & ResNet-18$\times$2  & 72.53          & \textbf{72.77} \\ 
                                & ResNet-18$\times$4  & 72.83          & \textbf{73.11} \\
                                & ResNet-18$\times$8  & 73.04          & \textbf{73.52} \\ \hline  
\multirow{2}{*}{Architecture}   & DeiT-Tiny           & 78.80          & \textbf{79.38} \\
                                & Pit-Tiny            & 78.50          & \textbf{78.77}      \\ \toprule  
\end{tabular}
	} }
	\label{table:offline}
% \end{small}
	% \vspace{-8pt}
\end{table*}


\newpage
\subsection{Performance of Student Distilled from a Constant Ahead Teacher}\label{sec: extend_time }
\begin{table*}[h]
% \begin{small}
\centering
    % \vspace{-0.2cm}
    \caption{Performance of ResNet-18 on CIFAR-100 distilled from trained teacher with a constant $s$ step ahead. There are no significant improvements over the online KD.
    } 
      %\vspace{-0.1cm}
	\smallskip
      \renewcommand\arraystretch{1.3}
    \small{
	\resizebox{0.60\textwidth}{!}{ 

\begin{tabular}{ccccccc}
\toprule
Interval  (epoch)  & 0     & 0.5   & 1     & 1.5   & 2     & 2.5   \\ \hline
CIFAR-100 & 71.05 & 70.67 & 70.85 & 70.69 & 71.04 & 70.78 \\ \toprule
\end{tabular}


	} }
	\label{table:time}
% \end{small}
	% \vspace{-8pt}
\end{table*}

Here we consider a naive baseline of implementing the proposed spacing effect. Specifically, we first train the teacher model for $s$ steps and then transfers knowledge to the student model at each step during the following training time. In other words, the teacher model keeps constant $s$ steps ahead of the student model. However, such a naive baseline exhibits no significant improvement over online KD (see Table~\ref{table:time}), consistent with our empirical analysis (see Fig.~\ref{fig:time}) and theoretical analysis (see Sec.~\ref{sec:kd_theory}): The teacher model of Spaced KD can provide a stable informative direction for optimizing the student model after each $s$ steps, whereas the teacher model of the naive baseline fails in this purpose due to its ongoing changes when optimizing the student model. Such different effects also suggest that the implementation of spacing effect is highly non-trivial and requires specialized design as in our Spaced KD.

% \subsection{Performance of Spaced KD using different loss function.}



\subsection{Performance of Spaced KD using different learning rate and batch size}\label{sec:hyperparameter}
\begin{figure} [h]
    \centering
    \includegraphics[width=0.7\linewidth]{./imgs/figs/fig5_hyperparameter.pdf}
    \caption{Hyperparameter validation for Spaced KD. Accuracy of different learning rate \textbf{(a)} and batch size \textbf{(b)} of gradient intervals. %Spaced KD is effective across all learning rates and large batch sizes.
    }
    \label{fig:hyperparameter}
\end{figure}


\subsection{Performance of Spaced KD on Different Image Corruption Attacks}\label{sec: corrupt}

Here we visualize 6 representative image corruption operations~\citep{michaelis2019dragon} applied to the images from the CIFAR-100 dataset~\citep{cifar100} to assess our models' robustness and generalization ability in Fig.~\ref{fig: attack}. The accuracy under adversarial attacks with more noise intensity levels is listed in Tab.~\ref{table:extend_attack}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{./imgs/figs/fig_attack.pdf}
    \caption{Image corruption operation. We choose 6 representative image corruption operations with different severity (1.0, 3.0, 5.0) and visualized images come from the CIFAR-100 test set.
    }
    \label{fig: attack}
\end{figure}


\begin{table*}[hb]
	\centering
    % \vspace{-0.2cm}
    \caption{Comparison of accuracy under image corruption attack (\%). $\Delta$ indicates Spaced KD's increased performance based on online KD. The results of 1.0 intensity can be seen in Tab.~\ref{table:attack}. 
    } 
      %\vspace{-0.1cm}
	\smallskip
      \renewcommand\arraystretch{1.3}
    \small{
	\resizebox{0.98\textwidth}{!}{ 
        \begin{tabular}{ccccccccccc}
        \toprule
        \multirow{2}{*}{Attack}            & \multirow{2}{*}{Noise Intensity} & \multicolumn{3}{c}{ResNet-18}        & \multicolumn{3}{c}{ResNet-50}        & \multicolumn{3}{c}{ResNet-101}       \\ \cline{3-11} 
                                           &                                  & w/o Ours & w/ Ours        & $\Delta$ & w/o Ours & w/ Ours        & $\Delta$ & w/o Ours & w/ Ours        & $\Delta$ \\ \hline
        \multirow{2}{*}{\texttt{impulse\_noise}}    & 3.0                              & 34.19    & \textbf{35.33} & 1.14     & 35.41    & \textbf{36.53} & 1.12     & 37.56    & \textbf{38.16} & 0.60     \\
                                           & 5.0                              & \textbf{12.54}    & 12.04          & -0.50    & 10.49    & \textbf{10.57} & 0.08     & \textbf{12.08}    & 11.39          & -0.69    \\ \hline %\cline{2-11} 
        \multirow{2}{*}{\texttt{zoom\_blur}}        & 3.0                              & 64.73    & \textbf{65.29} & 0.56     & 65.04    & \textbf{66.45} & 1.41     & 64.5     & \textbf{64.98} & 0.48     \\
                                           & 5.0                              & 61.02    & \textbf{61.53} & 0.51     & 61.36    & \textbf{62.67} & 1.31     & 61.32    & \textbf{62.18} & 0.86     \\ \hline %\cline{2-11} 
        \multirow{2}{*}{\texttt{snow}}              & 3.0                              & 44.48    & \textbf{45.42} & 0.94     & 46.91    & \textbf{47.17} & 0.26     & 44.5     & \textbf{45.87} & 1.37     \\
                                           & 5.0                              & 28.60    & \textbf{29.48} & 0.88     & \textbf{30.09}    & 29.71          & -0.38    & 30.09    & \textbf{30.75} & 0.66     \\ \hline %\cline{2-11} 
        \multirow{2}{*}{\texttt{frost}}             & 3.0                              & 42.40    & \textbf{43.10} & 0.70     & \textbf{44.87}    & 44.69          & -0.18    & 45.10    & \textbf{45.28} & 0.18     \\
                                           & 5.0                              & 37.80    & \textbf{39.47} & 1.67     & 39.26    & \textbf{39.97} & 0.71     & \textbf{41.24}    & 40.59          & -0.65    \\ \hline %\cline{2-11} 
        \multirow{2}{*}{\texttt{jpeg\_compression}} & 3.0                              & \textbf{33.23}    & 32.32          & -0.91    & 33.05    & \textbf{33.99} & 0.94     & 34.80    & \textbf{35.63} & 0.83     \\
                                           & 5.0                              & 20.75    & \textbf{21.32} & 0.57     & 20.29    & \textbf{20.86} & 0.57     & 21.55    & \textbf{22.29} & 0.74     \\ \hline %\cline{2-11} 
        \multirow{2}{*}{\texttt{brightness}}        & 3.0                              & 62.77    & \textbf{64.68} & 1.91     & 64.48    & \textbf{64.63} & 0.15     & 62.90    & \textbf{64.01} & 1.11     \\
                                           & 5.0                              & 54.11    & \textbf{54.56} & 0.45     & 55.34    & \textbf{55.46} & 0.12     & 54.47    & \textbf{55.71} & 1.24     \\ \toprule %\hline
        \end{tabular}
	} }
	\label{table:extend_attack}
	% \vspace{-0.5cm}
\end{table*}

\newpage
\subsection{Performance of Spaced KD after Adversarial Attack}\label{sec:adversarial}

\begin{table*}[ht]
% \begin{small}
\centering
    % \vspace{-0.2cm}
    \caption{Performance of Spaced KD on CIFAR-100 after an adversarial attack called BIM~\citep{kurakin2017adversarial}. Spaced KD is more robust than online KD.
    } 
      %\vspace{-0.1cm}
	\smallskip
      \renewcommand\arraystretch{1.3}
    \small{
	\resizebox{0.50\textwidth}{!}{ 

\begin{tabular}{cccc}
\toprule
Network       & ResNet-18   & ResNet-50   & ResNet-101    \\ \hline
w/o           & 31.33       & 31.32       & 31.70                \\
w/1.5         & \textbf{31.44}       & \textbf{31.70}       & \textbf{33.69}                \\
$\Delta$      & +0.11       & +0.38       & +1.99                \\ \toprule
\end{tabular}


	} }
	\label{table:adversarial}
% \end{small}
	% \vspace{-8pt}
\end{table*}

% Here we choose 5 representative loss functions applied to Spaced KD. L1 measures the mean absolute error; smooth L1 uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise; MSE (reduction=mean) measures the mean squared error; MSE (reduction=sum) measures the sum squared error; cross-entropy computes the cross-entropy loss.
% \input{SPACE/tabs/Extend_loss}

% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}

\subsection{Pseudo Code of Online KD, Self KD and Spaced KD}\label{sec: pseudo}

\begin{algorithm}[htb]
    \caption{Training Algorithm of Online KD}\label{alg:online kd}
    \textbf{Require}: student $f_\theta$, teacher $g_\phi$, dataset $\mathcal{D}_{\text{train}}$, KD loss weight $\alpha$, epoch number $E$\\
    \textbf{Ensure}: train both teacher and student using online knowledge distillation
    \begin{algorithmic}[1]
        \FOR{$1\leq e\leq E$ } 
            \FOR{($x_i, y_i) \in \mathcal{D}_{\text{train}}$}
                \STATE Update teacher $\phi \leftarrow \phi - \nabla_\phi l_{\text{task}}(g_\phi(x_i), y_i) $
                \STATE Update student $\theta \leftarrow \theta - \nabla_\theta \left[\alpha l_{\text{KD}}(f_\theta(x_i), g_\phi(x_i)) + (1-\alpha) l_{\text{task}}(f_\theta(x_i), y_i)\right]$
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}[htb]
    \caption{Training Algorithm of Online KD with Spaced KD}\label{alg:spaced kd}
    \textbf{Require}: student $f_\theta$, teacher $g_\phi$, dataset $\mathcal{D}_{\text{train}}$, KD loss weight $\alpha$, epoch number $E$, space interval $s$\\
    \textbf{Ensure}: train both teacher and student using spaced knowledge distillation
    \begin{algorithmic}[1]
        \STATE Initialize data index set: $\mathcal{R} \leftarrow \emptyset$
            \FOR{$1\leq e\leq E$ }
            \FOR{($x_i, y_i) \in \mathcal{D}_{\text{train}}$}
                \STATE $\mathcal{R} \leftarrow \mathcal{R}\cup \{i\}$
                \STATE Update teacher $\phi \leftarrow \phi - \nabla_\phi l_{\text{task}}(g_\phi(x_i), y_i) $
                \IF{$|\mathcal{R}| == s$}
                    \FOR{$j \in \mathcal{R}$}
                        \STATE Retrieve $(x_j, y_j) $ from $ \mathcal{D}_{\text{train}}$
                        \STATE Update student $\theta \leftarrow \theta - \nabla_\theta \left[\alpha l_{\text{KD}}(f_\theta(x_j), g_\phi(x_j)) + (1-\alpha) l_{\text{task}}(f_\theta(x_j), y_j)\right]$
                    \ENDFOR
                \ENDIF
                \STATE Clear index set: $\mathcal{R} \leftarrow \emptyset$
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}[htb]
    \caption{Training Algorithm of Self KD}\label{alg:self kd}
    \textbf{Require}: network $f_{\theta} = f_{\theta_1}\circ \cdots \circ f_{\theta_m}$ consisting of $m$ blocks, dataset $\mathcal{D}_{\text{train}}$, KD loss weight $\alpha$, epoch number $E$\\
    \textbf{Ensure}: train $f_{\theta}$ by distilling logits from the last block to the shallower blocks
    \begin{algorithmic}[1]
        \FOR{$1 \leq e \leq E$ }
        \FOR{$(x_i, y_i) \in \mathcal{D}_{\text{train}}$}
            \STATE Calculate loss $L = l_{\text{task}}(f_{\theta}(x_i), y_i)$
            \FOR{$1 \leq k < m$}
                \STATE $L\leftarrow L + \alpha l_{\text{KD}}(f_{\theta_1}\circ \cdots \circ f_{\theta_k}(x_i), f_{\theta}(x_i))+(1-\alpha)l_{\text{task}}(f_{\theta_1}\circ \cdots \circ f_{\theta_k}(x_i), y_i)$
            \ENDFOR
            \STATE Update network $\theta \leftarrow \theta - \nabla_{\theta}L$
        \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}[htb]
    \caption{Training Algorithm of Self KD with Spaced KD}\label{alg:spaced self kd}
    \textbf{Require}: network $f_{\theta} = f_{\theta_1}\circ \cdots \circ f_{\theta_m}$ consisting of $m$ blocks, dataset $\mathcal{D}_{\text{train}}$, KD loss weight $\alpha$, epoch number $E$, space interval $s$\\
    \textbf{Ensure}: train $f_\theta$ by distilling logits from the last block to shallower blocks in a spaced manner
    \begin{algorithmic}[1]
        \STATE Initialize data index set: $\mathcal{R} \leftarrow \emptyset$
        \FOR{$1\leq e\leq E$ }
        \FOR{$(x_i, y_i) \in \mathcal{D}_{\text{train}}$}
            \STATE $\mathcal{R} \leftarrow \mathcal{R}\cup \{i\}$
            \STATE Calculate loss $L = l_{\text{task}}(f_{\theta}(x_i), y_i)$
            \STATE Update network $\theta \leftarrow \theta - \nabla_{\theta}L$
            \IF{$|\mathcal{R}| == s$}
                \FOR{$j\in \mathcal{R}$}
                    \STATE Retrieve $(x_j, y_j) $ from $ \mathcal{D}_{\text{train}}$
                    \STATE Calculate loss $L^\prime = l_{\text{task}}(f_{\theta}(x_j), y_j)$
                    \FOR{$1\leq k < m$}
                        \STATE $L^{\prime}\leftarrow L^{\prime} + \alpha l_{\text{KD}}(f_{\theta_1}\circ \cdots \circ f_{\theta_k}(x_j), f_{\theta}(x_j))+(1-\alpha)l_{\text{task}}(f_{\theta_1}\circ \cdots \circ f_{\theta_k}(x_j), y_j)$
                    \ENDFOR
                    \STATE Update network $\theta \leftarrow \theta - \nabla_{\theta}L^\prime$
                \ENDFOR
            \ENDIF
            \STATE Clear index set: $\mathcal{R} \leftarrow \emptyset$
        \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}






%% ===== BIBLIOGRAPHY (INLINED) =====
\begin{thebibliography}{83}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adriana et~al.(2015)Adriana, Nicolas, Ebrahimi, Antoine, Carlo, and Yoshua]{adriana2015fitnets}
Adriana, R., Nicolas, B., Ebrahimi, K.~S., Antoine, C., Carlo, G., and Yoshua, B.
\newblock Fitnets: Hints for thin deep nets.
\newblock \emph{International Conference on Learning Representations}, 2\penalty0 (3):\penalty0 1, 2015.

\bibitem[Allen-Zhu \& Li(2020)Allen-Zhu and Li]{allen2020towards}
Allen-Zhu, Z. and Li, Y.
\newblock Towards understanding ensemble, knowledge distillation and self-distillation in deep learning.
\newblock \emph{arXiv preprint arXiv:2012.09816}, 2020.

\bibitem[Anderson et~al.(2008)Anderson, Jablonski, and Klimas]{anderson2008spaced}
Anderson, M.~J., Jablonski, S.~A., and Klimas, D.~B.
\newblock Spaced initial stimulus familiarization enhances novelty preference in long-evans rats.
\newblock \emph{Behavioural Processes}, 78\penalty0 (3):\penalty0 481--486, 2008.

\bibitem[Asif et~al.(2019)Asif, Tang, and Harrer]{asif2019ensemble}
Asif, U., Tang, J., and Harrer, S.
\newblock Ensemble knowledge distillation for learning improved and efficient networks.
\newblock \emph{arXiv preprint arXiv:1909.08097}, 2019.

\bibitem[Beck et~al.(2000)Beck, Schroeder, and Davis]{beck2000learning}
Beck, C., Schroeder, B., and Davis, R.~L.
\newblock Learning performance of normal and mutantdrosophila after repeated conditioning trials with discrete stimuli.
\newblock \emph{Journal of Neuroscience}, 20\penalty0 (8):\penalty0 2944--2953, 2000.

\bibitem[Bello-Medina et~al.(2013)Bello-Medina, S{\'a}nchez-Carrasco, Gonz{\'a}lez-Ornelas, Jeffery, and Ram{\'\i}rez-Amaya]{bello2013differential}
Bello-Medina, P.~C., S{\'a}nchez-Carrasco, L., Gonz{\'a}lez-Ornelas, N.~R., Jeffery, K.~J., and Ram{\'\i}rez-Amaya, V.
\newblock Differential effects of spaced vs. massed training in long-term object-identity and object-location recognition memory.
\newblock \emph{Behavioural Brain Research}, 250:\penalty0 102--113, 2013.

\bibitem[Blanc et~al.(2020)Blanc, Gupta, Valiant, and Valiant]{blanc2020implicit}
Blanc, G., Gupta, N., Valiant, G., and Valiant, P.
\newblock Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process.
\newblock In \emph{Conference on Learning Theory}, pp.\  483--513. PMLR, 2020.

\bibitem[Chen et~al.(2020)Chen, Mei, Wang, Feng, and Chen]{multiTeacher}
Chen, D., Mei, J.-P., Wang, C., Feng, Y., and Chen, C.
\newblock Online knowledge distillation with diverse peers.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~34, pp.\  3430--3437, 2020.

\bibitem[Cho \& Hariharan(2019)Cho and Hariharan]{earlystop}
Cho, J.~H. and Hariharan, B.
\newblock On the efficacy of knowledge distillation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, October 2019.

\bibitem[Damian et~al.(2021)Damian, Ma, and Lee]{damian2021label}
Damian, A., Ma, T., and Lee, J.~D.
\newblock Label noise sgd provably prefers flat global minimizers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 27449--27461, 2021.

\bibitem[Das \& Sanghavi(2023)Das and Sanghavi]{das2023understanding}
Das, R. and Sanghavi, S.
\newblock Understanding self-distillation in the presence of label noise.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7102--7140. PMLR, 2023.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1019--1028. PMLR, 2017.

\bibitem[Donovan \& Radosevich(1999)Donovan and Radosevich]{donovan1999meta}
Donovan, J.~J. and Radosevich, D.~J.
\newblock A meta-analytic review of the distribution of practice effect: Now you see it, now you don't.
\newblock \emph{Journal of Applied Psychology}, 84\penalty0 (5):\penalty0 795, 1999.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Ebbinghaus(2013)]{ebbingham1913memory}
Ebbinghaus, H.
\newblock Memory: A contribution to experimental psychology.
\newblock \emph{Annals of Neurosciences}, 20\penalty0 (4):\penalty0 155, 2013.

\bibitem[Frankle et~al.()Frankle, Schwab, and Morcos]{EarlyPhase}
Frankle, J., Schwab, D.~J., and Morcos, A.~S.
\newblock The early phase of neural network training.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[Furlanello et~al.(2018)Furlanello, Lipton, Tschannen, Itti, and Anandkumar]{furlanello2018born}
Furlanello, T., Lipton, Z., Tschannen, M., Itti, L., and Anandkumar, A.
\newblock Born again neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1607--1616. PMLR, 2018.

\bibitem[Gao et~al.(2018)Gao, Shen, Li, Yan, Wan, Lin, Loy, and Tang]{gao2018embarrassingly}
Gao, M., Shen, Y., Li, Q., Yan, J., Wan, L., Lin, D., Loy, C.~C., and Tang, X.
\newblock An embarrassingly simple approach for knowledge distillation.
\newblock \emph{arXiv preprint arXiv:1812.01819}, 2018.

\bibitem[Gluckman et~al.(2014)Gluckman, Vlach, and Sandhofer]{gluckman2014spacing}
Gluckman, M., Vlach, H.~A., and Sandhofer, C.~M.
\newblock Spacing simultaneously promotes multiple forms of learning in children's science curriculum.
\newblock \emph{Applied Cognitive Psychology}, 28\penalty0 (2):\penalty0 266--273, 2014.

\bibitem[Gou et~al.(2021)Gou, Yu, Maybank, and Tao]{review_KDSurvey}
Gou, J., Yu, B., Maybank, S.~J., and Tao, D.
\newblock Knowledge distillation: A survey.
\newblock \emph{International Journal of Computer Vision}, 129\penalty0 (6):\penalty0 1789--1819, 2021.

\bibitem[Gu \& Tresp(2020)Gu and Tresp]{gu2020search}
Gu, J. and Tresp, V.
\newblock Search for better students to learn distilled knowledge.
\newblock \emph{arXiv preprint arXiv:2001.11612}, 2020.

\bibitem[He et~al.(2019)He, Huang, and Yuan]{he2019asymmetric}
He, H., Huang, G., and Yuan, Y.
\newblock Asymmetric valleys: Beyond sharp and flat local minima.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  770--778, 2016.

\bibitem[Heo et~al.(2021)Heo, Yun, Han, Chun, Choe, and Oh]{heo2021rethinking}
Heo, B., Yun, S., Han, D., Chun, S., Choe, J., and Oh, S.~J.
\newblock Rethinking spatial dimensions of vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  11936--11945, 2021.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hochreiter \& Schmidhuber(1994)Hochreiter and Schmidhuber]{hochreiter1994simplifying}
Hochreiter, S. and Schmidhuber, J.
\newblock Simplifying neural nets by discovering flat minima.
\newblock \emph{Advances in Neural Information Processing Systems}, 7, 1994.

\bibitem[Huang \& Wang(2017)Huang and Wang]{huang2017like}
Huang, Z. and Wang, N.
\newblock Like what you like: Knowledge distill via neuron selectivity transfer.
\newblock \emph{arXiv preprint arXiv:1707.01219}, 2017.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson]{izmailov2018averaging}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Jastrzebski et~al.(2019)Jastrzebski, Kenton, Ballas, Fischer, Bengio, and Storkey]{Jastrzebski2019Sharpest}
Jastrzebski, S., Kenton, Z., Ballas, N., Fischer, A., Bengio, Y., and Storkey, A.~J.
\newblock On the relation between the sharpest directions of dnn loss and the sgd step length.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kang et~al.(2020)Kang, Mun, and Han]{kang2020towards}
Kang, M., Mun, J., and Han, B.
\newblock Towards oracle knowledge distillation with neural architecture search.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~34, pp.\  4404--4411, 2020.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kim et~al.(2021)Kim, Ji, Yoon, and Hwang]{PSKD}
Kim, K., Ji, B., Yoon, D., and Hwang, S.
\newblock Self-knowledge distillation with progressive refinement of targets.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  6567--6576, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar100}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kurakin et~al.(2017)Kurakin, Goodfellow, and Bengio]{kurakin2017adversarial}
Kurakin, A., Goodfellow, I., and Bengio, S.
\newblock Adversarial examples in the physical world, 2017.
\newblock URL \url{https://arxiv.org/abs/1607.02533}.

\bibitem[Landauer(1969)]{landauer1969reinforcement}
Landauer, T.~K.
\newblock Reinforcement as consolidation.
\newblock \emph{Psychological Review}, 76\penalty0 (1):\penalty0 82, 1969.

\bibitem[Lee et~al.(2019)Lee, Hwang, and Shin]{lee2019rethinking}
Lee, H., Hwang, S.~J., and Shin, J.
\newblock Rethinking data augmentation: Self-supervision and self-distillation.
\newblock 2019.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and Gur-Ari]{lewkowycz2020large}
Lewkowycz, A., Bahri, Y., Dyer, E., Sohl-Dickstein, J., and Gur-Ari, G.
\newblock The large learning rate phase of deep learning: the catapult mechanism.
\newblock \emph{arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Wang, and Qi]{li2022online}
Li, C., Wang, Z., and Qi, H.
\newblock Online knowledge distillation by temporal-spatial boosting.
\newblock In \emph{Proceedings of the IEEE/CVF winter conference on applications of computer vision}, pp.\  197--206, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Yu, Wang, Yuan, Song, and Chen]{li2022locality}
Li, K., Yu, R., Wang, Z., Yuan, L., Song, G., and Chen, J.
\newblock Locality guidance for improving vision transformers on tiny datasets.
\newblock In \emph{European Conference on Computer Vision}, pp.\  110--127. Springer, 2022{\natexlab{b}}.

\bibitem[Li \& Jin(2022)Li and Jin]{li2022shadow}
Li, L. and Jin, Z.
\newblock Shadow knowledge distillation: Bridging offline and online knowledge transfer.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 635--649, 2022.

\bibitem[Li et~al.(2022{\natexlab{c}})Li, Lin, Wang, Wu, Tian, Shao, and Ji]{li2022distilling}
Li, S., Lin, M., Wang, Y., Wu, Y., Tian, Y., Shao, L., and Ji, R.
\newblock Distilling a powerful student model via online knowledge distillation.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 34\penalty0 (11):\penalty0 8743--8752, 2022{\natexlab{c}}.

\bibitem[Li et~al.(2023)Li, Li, Yang, Zhao, Song, Luo, Li, and Yang]{li2023curriculum}
Li, Z., Li, X., Yang, L., Zhao, B., Song, R., Luo, L., Li, J., and Yang, J.
\newblock Curriculum temperature for knowledge distillation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pp.\  1504--1512, 2023.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Jia, Tan, Vemulapalli, Zhu, Green, and Wang]{KnowledgeGap_2020_CVPR}
Liu, Y., Jia, X., Tan, M., Vemulapalli, R., Zhu, Y., Green, B., and Wang, X.
\newblock Search to distill: Pearls are everywhere but not the eyes.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, June 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Jia, Tan, Vemulapalli, Zhu, Green, and Wang]{liu2020search}
Liu, Y., Jia, X., Tan, M., Vemulapalli, R., Zhu, Y., Green, B., and Wang, X.
\newblock Search to distill: Pearls are everywhere but not the eyes.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  7539--7548, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2021)Liu, Sangineto, Bi, Sebe, Lepri, and Nadai]{liu2021efficient}
Liu, Y., Sangineto, E., Bi, W., Sebe, N., Lepri, B., and Nadai, M.
\newblock Efficient training of visual transformers with small datasets.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 23818--23830, 2021.

\bibitem[Loshchilov \& Hutter(2017{\natexlab{a}})Loshchilov and Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017{\natexlab{a}}.

\bibitem[Loshchilov \& Hutter(2017{\natexlab{b}})Loshchilov and Hutter]{loshchilov2017sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts, 2017{\natexlab{b}}.

\bibitem[Medin(1974)]{medin1974comparative}
Medin, D.~L.
\newblock The comparative study of memory.
\newblock \emph{Journal of Human Evolution}, 3\penalty0 (6):\penalty0 455--463, 1974.

\bibitem[Menzel et~al.(2001)Menzel, Manz, Menzel, and Greggers]{menzel2001massed}
Menzel, R., Manz, G., Menzel, R., and Greggers, U.
\newblock Massed and spaced learning in honeybees: the role of cs, us, the intertrial interval, and the test interval.
\newblock \emph{Learning \& Memory}, 8\penalty0 (4):\penalty0 198--208, 2001.

\bibitem[Michaelis et~al.(2019)Michaelis, Mitzkus, Geirhos, Rusak, Bringmann, Ecker, Bethge, and Brendel]{michaelis2019dragon}
Michaelis, C., Mitzkus, B., Geirhos, R., Rusak, E., Bringmann, O., Ecker, A.~S., Bethge, M., and Brendel, W.
\newblock Benchmarking robustness in object detection: Autonomous driving when winter is coming.
\newblock \emph{arXiv preprint arXiv:1907.07484}, 2019.

\bibitem[Mirzadeh et~al.(2020{\natexlab{a}})Mirzadeh, Farajtabar, Li, Levine, Matsukawa, and Ghasemzadeh]{mirzadeh2020improved}
Mirzadeh, S.~I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., and Ghasemzadeh, H.
\newblock Improved knowledge distillation via teacher assistant.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~34, pp.\  5191--5198, 2020{\natexlab{a}}.

\bibitem[Mirzadeh et~al.(2020{\natexlab{b}})Mirzadeh, Farajtabar, Li, Levine, Matsukawa, and Ghasemzadeh]{teacherassistant}
Mirzadeh, S.~I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., and Ghasemzadeh, H.
\newblock Improved knowledge distillation via teacher assistant.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~34, pp.\  5191--5198, 2020{\natexlab{b}}.

\bibitem[Mobahi et~al.(2020)Mobahi, Farajtabar, and Bartlett]{mobahi2020self}
Mobahi, H., Farajtabar, M., and Bartlett, P.
\newblock Self-distillation amplifies regularization in hilbert space.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3351--3361, 2020.

\bibitem[Pagani et~al.(2009)Pagani, Oishi, Gelb, and Zhong]{pagani2009phosphatase}
Pagani, M.~R., Oishi, K., Gelb, B.~D., and Zhong, Y.
\newblock The phosphatase shp2 regulates the spacing effect for long-term memory induction.
\newblock \emph{Cell}, 139\penalty0 (1):\penalty0 186--198, 2009.

\bibitem[Peterson(1966)]{peterson1966short}
Peterson, L.~R.
\newblock Short-term verbal memory and learning.
\newblock \emph{Psychological Review}, 73\penalty0 (3):\penalty0 193, 1966.

\bibitem[Qian et~al.(2022)Qian, Wang, Yin, Hong, and Wang]{qian2022switchable}
Qian, B., Wang, Y., Yin, H., Hong, R., and Wang, M.
\newblock Switchable online knowledge distillation.
\newblock In \emph{European Conference on Computer Vision}, pp.\  449--466. Springer, 2022.

\bibitem[Rezagholizadeh et~al.(2021)Rezagholizadeh, Jafari, Salad, Sharma, Pasand, and Ghodsi]{rezagholizadeh2021pro}
Rezagholizadeh, M., Jafari, A., Salad, P., Sharma, P., Pasand, A.~S., and Ghodsi, A.
\newblock Pro-kd: Progressive distillation by following the footsteps of the teacher.
\newblock \emph{arXiv preprint arXiv:2110.08532}, 2021.

\bibitem[Robbins \& Bush(1973)Robbins and Bush]{robbins1973memory}
Robbins, D. and Bush, C.~T.
\newblock Memory in great apes.
\newblock \emph{Journal of Experimental Psychology}, 97\penalty0 (3):\penalty0 344, 1973.

\bibitem[Roediger \& Byrne(2008)Roediger and Byrne]{roediger2008learning}
Roediger, H.~L. and Byrne, J.
\newblock Learning and memory: A comprehensive reference (vol. 2).
\newblock \emph{Cognitive Psychology of Memory}, 2008.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, et~al.]{imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 115:\penalty0 211--252, 2015.

\bibitem[Shea et~al.(2000)Shea, Lai, Black, and Park]{shea2000spacing}
Shea, C.~H., Lai, Q., Black, C., and Park, J.-H.
\newblock Spacing practice sessions across days benefits the learning of motor skills.
\newblock \emph{Human Movement Science}, 19\penalty0 (5):\penalty0 737--760, 2000.

\bibitem[Shen et~al.(2022)Shen, Xu, Yang, Li, and Guo]{DLB}
Shen, Y., Xu, L., Yang, Y., Li, Y., and Guo, Y.
\newblock Self-distillation from the last mini-batch for consistency regularization.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  11943--11952, 2022.

\bibitem[Shi et~al.(2021)Shi, Song, Zhou, Li, and Li]{shi2021follow}
Shi, W., Song, Y., Zhou, H., Li, B., and Li, L.
\newblock Follow your path: a progressive method for knowledge distillation.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part III 21}, pp.\  596--611. Springer, 2021.

\bibitem[Smolen et~al.(2016)Smolen, Zhang, and Byrne]{smolen2016right}
Smolen, P., Zhang, Y., and Byrne, J.~H.
\newblock The right time to learn: mechanisms and optimization of spaced learning.
\newblock \emph{Nature Reviews Neuroscience}, 17\penalty0 (2):\penalty0 77--88, 2016.

\bibitem[Sobel et~al.(2011)Sobel, Cepeda, and Kapler]{sobel2011spacing}
Sobel, H.~S., Cepeda, N.~J., and Kapler, I.~V.
\newblock Spacing effects in real-world classroom vocabulary learning.
\newblock \emph{Applied Cognitive Psychology}, 25\penalty0 (5):\penalty0 763--767, 2011.

\bibitem[Sun et~al.(2024)Sun, Ren, Li, Wang, and Cao]{sun2024logit}
Sun, S., Ren, W., Li, J., Wang, R., and Cao, X.
\newblock Logit standardization in knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2403.01427}, 2024.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and Hinton]{sutskever2013importance}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1139--1147. PMLR, 2013.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and J{\'e}gou]{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou, H.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10347--10357. PMLR, 2021.

\bibitem[Vlach(2014)]{vlach2014spacing}
Vlach, H.~A.
\newblock The spacing effect in children's generalization of knowledge: Allowing children time to forget promotes their ability to learn.
\newblock \emph{Child Development Perspectives}, 8\penalty0 (3):\penalty0 163--168, 2014.

\bibitem[Wang \& Yoon(2021)Wang and Yoon]{review_ST}
Wang, L. and Yoon, K.-J.
\newblock Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44\penalty0 (6):\penalty0 3048--3068, 2021.

\bibitem[Wickelgren(1972)]{wickelgren1972trace}
Wickelgren, W.~A.
\newblock Trace resistance and the decay of long-term memory.
\newblock \emph{Journal of Mathematical Psychology}, 9\penalty0 (4):\penalty0 418--455, 1972.

\bibitem[Wu \& Gong(2021)Wu and Gong]{wu2021peer}
Wu, G. and Gong, S.
\newblock Peer collaborative learning for online knowledge distillation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  10302--10310, 2021.

\bibitem[Wu et~al.()Wu, Wang, and Su]{wu_alignment_2022}
Wu, L., Wang, M., and Su, W.
\newblock The alignment property of {SGD} noise and how it helps select flat minima: A stability analysis.
\newblock pp.\  4680--4693.

\bibitem[Xie et~al.(2020)Xie, Sato, and Sugiyama]{xie2020diffusion}
Xie, Z., Sato, I., and Sugiyama, M.
\newblock A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima.
\newblock \emph{arXiv preprint arXiv:2002.03495}, 2020.

\bibitem[Yan et~al.(2024)Yan, Wang, Ma, and Zhong]{yan2024orchestrate}
Yan, H., Wang, L., Ma, K., and Zhong, Y.
\newblock Orchestrate latent expertise: Advancing online continual learning with multi-level supervision and reverse self-distillation.
\newblock \emph{arXiv preprint arXiv:2404.00417}, 2024.

\bibitem[Yang et~al.(2019)Yang, Xie, Su, and Yuille]{yang2019snapshot}
Yang, C., Xie, L., Su, C., and Yuille, A.~L.
\newblock Snapshot distillation: Teacher-student optimization in one generation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  2859--2868, 2019.

\bibitem[Zhai et~al.(2019)Zhai, Chen, Tung, He, Nawhal, and Mori]{zhai2019lifelong}
Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M., and Mori, G.
\newblock Lifelong gan: Continual learning for conditional image generation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  2759--2768, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Song, Gao, Chen, Bao, and Ma]{self-kd}
Zhang, L., Song, J., Gao, A., Chen, J., Bao, C., and Ma, K.
\newblock Be your own teacher: Improve the performance of convolutional neural networks via self distillation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, October 2019.

\bibitem[Zhang et~al.(2018)Zhang, Xiang, Hospedales, and Lu]{deepMutual}
Zhang, Y., Xiang, T., Hospedales, T.~M., and Lu, H.
\newblock Deep mutual learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  4320--4328, 2018.

\bibitem[Zhang \& Sabuncu(2020)Zhang and Sabuncu]{zhang2020self}
Zhang, Z. and Sabuncu, M.
\newblock Self-distillation as instance-specific label smoothing.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 2184--2195, 2020.

\bibitem[Zhao et~al.(2023)Zhao, Cui, Song, and Liang]{zhao2023dot}
Zhao, B., Cui, Q., Song, R., and Liang, J.
\newblock Dot: A distillation-oriented trainer.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  6189--6198, 2023.

\bibitem[Zhou et~al.(2020)Zhou, Feng, Ma, Xiong, Hoi, et~al.]{zhou2020towards}
Zhou, P., Feng, J., Ma, C., Xiong, C., Hoi, S. C.~H., et~al.
\newblock Towards theoretically understanding why sgd generalizes better than adam in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 21285--21296, 2020.

\bibitem[Zhu et~al.(2018)Zhu, Gong, et~al.]{ONE}
Zhu, X., Gong, S., et~al.
\newblock Knowledge distillation by on-the-fly native ensemble.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\end{thebibliography}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.


%% ===== INCLUDED FROM: imgs/Ucurve.tex =====
% This content was originally in imgs/Ucurve.tex
\begin{figure*}
    %\vspace{-.1cm}
    \centering
    \includegraphics[width=0.8\linewidth]{SPACE/imgs/figs/fig2_Ucurve.pdf}
    \vspace{-.2cm}
    \caption{Alignment of spaced learning in BNNs and DNNs. \textbf{(a)} Computational cognitive model of spaced learning, modified from~\cite{landauer1969reinforcement}. \textbf{(b)} Overall performance of Spaced KD from different networks and benchmarks. R18: ResNet-18; R50: ResNet-50; R101: ResNet-101; C100: CIFAR-100; T200: Tiny-ImageNet. \textbf{(c)} Quadratic polynomial fitting of all performance from \textbf{(b)}. %All of them show that spaced training with appropriate interval can enhance learning performance. 
    %Depiction of the cognitive model was modified from~\cite{landauer1969reinforcement}.
    }
    \label{fig:main_result}
     \vspace{-.3cm}
\end{figure*}


% /imgs/figs/\begin{figure}[t]
%     \centering
%     \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{SPACE/imgs/figs/fig1_main_result_model.png}
%     \caption{Cognitive model}
%     \end{subfigure}
%     \begin{subfigure}{0.4\linewidth}
%     \includegraphics[width=\linewidth]{SPACE/imgs/figs/fig1_main_result.png}
%     \caption{Spaced KD}
%     \end{subfigure}
% \caption{\textbf{Diagram of computational cognitive model (a) of spaced learning and increased performance (b) of spaced KD.} Both of them shows spaced training is more effective than massed training, and there may be an optimal interval. The cognitive model was modified from Landauer~\citep{landauer1969reinforcement}.
% }
% \label{fig:main_result}
% \vspace{-10pt}
% \end{figure}


%% ===== INCLUDED FROM: imgs/attack.tex =====
% This content was originally in imgs/attack.tex
\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{SPACE/imgs/figs/fig_attack.pdf}
    \caption{Image corruption operation. We choose 6 representative image corruption operations with different severity (1.0, 3.0, 5.0) and visualized images come from the CIFAR-100 test set.
    }
    \label{fig: attack}
\end{figure}

%% ===== INCLUDED FROM: imgs/diagram.tex =====
% This content was originally in imgs/diagram.tex
\begin{figure}
    \centering
     \vspace{+.2cm}
    \includegraphics[width=0.8\linewidth]{SPACE/imgs/figs/fig1_diagram.pdf}
    \vspace{-.1cm}
    \caption{\textbf{Diagram of Spaced KD.} In online KD, the teacher and student are two individual networks. In self KD, we follow the prior work~\citep{self-kd} that distills knowledge from the deepest layer to the shallower layers of the same network. In Spaced KD, we train the teacher with a controllable space interval steps ahead and then distill its knowledge to the student network.
    %Thus, the teacher is the deepest layer of the network and the students are the shallow layers along with their corresponding classifiers.
    }
    \label{fig:diagram}
    \vspace{-.4cm}
\end{figure}

%% ===== INCLUDED FROM: imgs/hyperparameter.tex =====
% This content was originally in imgs/hyperparameter.tex
\begin{figure} [h]
    \centering
    \includegraphics[width=0.7\linewidth]{SPACE/imgs/figs/fig5_hyperparameter.pdf}
    \caption{Hyperparameter validation for Spaced KD. Accuracy of different learning rate \textbf{(a)} and batch size \textbf{(b)} of gradient intervals. %Spaced KD is effective across all learning rates and large batch sizes.
    }
    \label{fig:hyperparameter}
\end{figure}

%% ===== INCLUDED FROM: imgs/imagenet.tex =====
% This content was originally in imgs/imagenet.tex
\begin{figure} [h]
    \centering
    \includegraphics[width=0.65\linewidth]{SPACE/imgs/figs/fig6_ImageNet.pdf}
    \caption{Training curve of ResNet-18 and ImageNet-1k (space interval 1.5 epochs).% Spaced KD remarkably improves the performance in the entire training process.
    }
    \label{fig:imagenet}
    \vspace{-.2cm}
\end{figure}

%% ===== INCLUDED FROM: imgs/noise.tex =====
% This content was originally in imgs/noise.tex
\begin{figure}[htb]
   % \vspace{-0.2cm}
    \centering
    \includegraphics[width=0.75\linewidth]{SPACE/imgs/figs/fig4_noise.pdf}
    \vspace{-0.01cm}
    \caption{Impact of Gaussian noise on performance. 
    % Under the same noise perturbations, the network trained with Spaced KD exhibits lower loss changes and higher accuracy.
    }
    \label{fig:noise}
    \vspace{-0.01cm}
\end{figure}
% Comparison of training accuracy and loss with increasing Gaussian noise.

%% ===== INCLUDED FROM: imgs/time.tex =====
% This content was originally in imgs/time.tex
\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1.0\linewidth]{SPACE/imgs/figs/fig3_time.pdf}
    % \vspace{0 pt}
    \caption{Impact of different initiating times of Spaced KD ($s=1.5$), which is introduced \textbf{(a)} for constant 10 training epochs or \textbf{(b)} till the end of training. %Spaced KD is effective at the later stage of training, rather than the early stage.
    }
    \label{fig:time}
    % \vspace{-.0cm}
\end{figure}


% Both results indicate that initiating Spaced KD in the later stages of training is more beneficial for improving the performance of the student network. This suggests that in KD learning, not only the intervals between learning sessions are important, but the timing of when learning occurs is equally crucial.
