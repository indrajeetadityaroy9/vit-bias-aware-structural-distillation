data:
  augmentation:
    random_crop: true
    random_flip: true
    color_jitter: true
    cutout: true
    randaugment: true
    randaugment_n: 2
    randaugment_m: 9
    mixup: true
    mixup_alpha: 0.8
    cutmix: true  # GPU-side CutMix (no dataset wrapper limitation)
    cutmix_alpha: 1.0
  batch_size: 4096  # 2x H100 optimized (saturate Tensor Cores)
  data_path: ./data
  dataset: cifar
  normalization:
    mean:
    - 0.4914
    - 0.4822
    - 0.4465
    std:
    - 0.2470
    - 0.2435
    - 0.2616
  num_workers: 16  # High CPU core count available (52 vCPUs)
  persistent_workers: true
  pin_memory: true
  prefetch_factor: 4
  # H100 optimizations
  gpu_augmentation: true  # Apply MixUp/CutMix on GPU (avoid CPU bottleneck)
  drop_last: true  # Required for CUDA Graphs (fixed batch size)

device: cuda
experiment_name: deit_cifar_improved
output_dir: ./outputs
seed: 42

logging:
  log_dir: ./logs
  log_level: INFO
  save_frequency: 10
  track_grad_norm: true
  wandb: false
  wandb_entity: null
  wandb_project: adaptive-cnn

model:
  model_type: deit
  in_channels: 3
  num_classes: 10
  architecture: []
  classifier_layers: []
  dropout: 0.0
  use_se: true  # Must match teacher config

vit:
  variant: tiny
  img_size: 32
  patch_size: 4  # Keep at 4 for CIFAR (32/4 = 64 patches is reasonable)
  embed_dim: 192
  depth: 12
  num_heads: 3
  mlp_ratio: 2.0  # Reduced from 4.0 for better efficiency
  drop_rate: 0.0
  attn_drop_rate: 0.0
  drop_path_rate: 0.1
  distillation: true
  convert_grayscale: false
  # New generalized improvements
  use_conv_stem: true  # Hybrid conv stem for better local feature extraction
  cls_token_dropout: 0.1  # Replace cls token with patch mean 10% of time
  inference_mode: avg  # Use average of cls and dist outputs

distillation:
  teacher_checkpoint: ./outputs/cifar_improved/checkpoints/best_model.pth
  teacher_model_type: adaptive_cnn
  distillation_type: hard
  alpha: 0.6  # Base alpha (used when schedule is constant)
  tau: 3.0
  distillation_warmup_epochs: 5
  # Alpha scheduling: ramp up distillation contribution
  alpha_schedule: cosine
  alpha_start: 0.0
  alpha_end: 0.6

training:
  num_epochs: 100
  learning_rate: 0.004  # Scaled for batch_size=4096 (sqrt scaling: 0.002 * sqrt(4096/1024))
  weight_decay: 0.05
  optimizer: adamw
  scheduler: cosine
  warmup_epochs: 10
  gradient_clip_val: 1.0
  gradient_accumulation_steps: 1
  use_amp: true
  amp_backend: native
  early_stopping: true
  early_stopping_patience: 20
  early_stopping_min_delta: 0.0005
  lr_scheduler_params:
    T_max: 100
    eta_min: 0.00001
  label_smoothing: 0.1
  use_swa: false
  swa_start_epoch: 0.75
  swa_lr: 0.0001
  # H100 optimizations
  use_bf16: true  # Native BF16 (no GradScaler needed)
  use_compile: true  # torch.compile for graph optimization
  compile_mode: max-autotune  # Optimal Triton kernels for H100
  use_fused_optimizer: true  # Fused AdamW kernels
  use_tf32: true  # TF32 matmul acceleration
  use_cuda_graphs: false  # Enable for small batches (experimental)
