# Global defaults for all experiments
# These values are merged with specific configs (specific values override defaults)

# Reproducibility
seed: 42
device: cuda

# Logging defaults
logging:
  log_dir: ./logs
  log_level: INFO
  save_frequency: 10
  track_grad_norm: true
  wandb: false
  wandb_entity: null
  wandb_project: vit-distillation

# Training defaults (H100 optimized)
training:
  optimizer: adamw
  scheduler: cosine
  warmup_epochs: 10
  gradient_clip_val: 1.0
  gradient_accumulation_steps: 1
  use_amp: true
  amp_backend: native
  early_stopping: true
  early_stopping_patience: 20
  early_stopping_min_delta: 0.001
  label_smoothing: 0.1
  use_swa: true
  swa_start_epoch: 0.8
  swa_lr: 0.0001
  # H100 optimization flags
  use_bf16: true              # Native BF16 (no GradScaler needed)
  use_compile: true           # torch.compile for graph optimization
  compile_mode: max-autotune  # Optimal Triton kernels for H100
  use_fused_optimizer: true   # Fused AdamW/SGD kernels
  use_tf32: true              # TF32 matmul acceleration
  use_cuda_graphs: false      # Disabled by default (experimental)

# Data loading defaults
data:
  num_workers: 16
  persistent_workers: true
  pin_memory: true
  prefetch_factor: 4
  data_path: ./data
  # H100 optimizations
  gpu_augmentation: true      # Apply MixUp/CutMix on GPU
  drop_last: true             # Required for CUDA Graphs (fixed batch size)

# Model defaults
model:
  dropout: 0.1
  use_se: true
  pretrained: false
  drop_path_rate: 0.1

# Output
output_dir: ./outputs
